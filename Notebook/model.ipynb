{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flu Modeling Notebook ðŸ¦ \n",
    "\n",
    "### Step One: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>WEEK</th>\n",
       "      <th>% WEIGHTED ILI</th>\n",
       "      <th>REGIONAL ILI BASELINE</th>\n",
       "      <th>ABOVE BASELINE</th>\n",
       "      <th>IS FLUWEEK</th>\n",
       "      <th>AGE 0-4</th>\n",
       "      <th>AGE 5-24</th>\n",
       "      <th>AGE 25-64</th>\n",
       "      <th>...</th>\n",
       "      <th>COOLING DAYS MONTHLY</th>\n",
       "      <th>HEATING DAYS MONTHLY</th>\n",
       "      <th>PRECIPITATION MONTHLY</th>\n",
       "      <th>AVG TEMP MONTHLY</th>\n",
       "      <th>POPULATION</th>\n",
       "      <th>VAXEFFECTIVENESS</th>\n",
       "      <th>VACCINERATENATIONAL</th>\n",
       "      <th>AWND</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>TAVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-10-05</td>\n",
       "      <td>2003</td>\n",
       "      <td>40</td>\n",
       "      <td>0.572031</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.2</td>\n",
       "      <td>150.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14182454</td>\n",
       "      <td>52</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2.805714</td>\n",
       "      <td>1.808571</td>\n",
       "      <td>11.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-10-12</td>\n",
       "      <td>2003</td>\n",
       "      <td>41</td>\n",
       "      <td>0.449279</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.2</td>\n",
       "      <td>150.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14182454</td>\n",
       "      <td>52</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2.480000</td>\n",
       "      <td>2.726190</td>\n",
       "      <td>12.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-10-19</td>\n",
       "      <td>2003</td>\n",
       "      <td>42</td>\n",
       "      <td>0.613789</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.2</td>\n",
       "      <td>150.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14182454</td>\n",
       "      <td>52</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.634286</td>\n",
       "      <td>5.311905</td>\n",
       "      <td>10.291429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-10-26</td>\n",
       "      <td>2003</td>\n",
       "      <td>43</td>\n",
       "      <td>0.661170</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>56</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.2</td>\n",
       "      <td>150.1</td>\n",
       "      <td>9.7</td>\n",
       "      <td>14182454</td>\n",
       "      <td>52</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.974286</td>\n",
       "      <td>2.473333</td>\n",
       "      <td>7.068571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-11-02</td>\n",
       "      <td>2003</td>\n",
       "      <td>44</td>\n",
       "      <td>0.760914</td>\n",
       "      <td>0.782203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>74</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>375.4</td>\n",
       "      <td>77.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>14182454</td>\n",
       "      <td>52</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3.291429</td>\n",
       "      <td>9.711905</td>\n",
       "      <td>12.899286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  YEAR  WEEK  % WEIGHTED ILI  REGIONAL ILI BASELINE  \\\n",
       "0  2003-10-05  2003    40        0.572031               0.782203   \n",
       "1  2003-10-12  2003    41        0.449279               0.782203   \n",
       "2  2003-10-19  2003    42        0.613789               0.782203   \n",
       "3  2003-10-26  2003    43        0.661170               0.782203   \n",
       "4  2003-11-02  2003    44        0.760914               0.782203   \n",
       "\n",
       "   ABOVE BASELINE  IS FLUWEEK  AGE 0-4  AGE 5-24  AGE 25-64  ...  \\\n",
       "0               0           0       14        40         27  ...   \n",
       "1               0           0        6        35         35  ...   \n",
       "2               0           0       19        60         32  ...   \n",
       "3               0           0       32        56         33  ...   \n",
       "4               0           0       31        74         38  ...   \n",
       "\n",
       "   COOLING DAYS MONTHLY  HEATING DAYS MONTHLY  PRECIPITATION MONTHLY  \\\n",
       "0                   0.0                 266.2                  150.1   \n",
       "1                   0.0                 266.2                  150.1   \n",
       "2                   0.0                 266.2                  150.1   \n",
       "3                   0.0                 266.2                  150.1   \n",
       "4                   0.0                 375.4                   77.7   \n",
       "\n",
       "   AVG TEMP MONTHLY  POPULATION  VAXEFFECTIVENESS  VACCINERATENATIONAL  \\\n",
       "0               9.7    14182454                52                 12.7   \n",
       "1               9.7    14182454                52                 12.7   \n",
       "2               9.7    14182454                52                 12.7   \n",
       "3               9.7    14182454                52                 12.7   \n",
       "4               5.6    14182454                52                 12.7   \n",
       "\n",
       "       AWND      PRCP       TAVG  \n",
       "0  2.805714  1.808571  11.100000  \n",
       "1  2.480000  2.726190  12.740000  \n",
       "2  3.634286  5.311905  10.291429  \n",
       "3  3.974286  2.473333   7.068571  \n",
       "4  3.291429  9.711905  12.899286  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../Data/more_combined_data.csv\"\n",
    "scale = StandardScaler()\n",
    "df_show = pd.read_csv(path)\n",
    "df_show.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE                            object\n",
       "YEAR                             int64\n",
       "WEEK                             int64\n",
       "% WEIGHTED ILI                 float64\n",
       "REGIONAL ILI BASELINE          float64\n",
       "ABOVE BASELINE                   int64\n",
       "IS FLUWEEK                       int64\n",
       "AGE 0-4                          int64\n",
       "AGE 5-24                         int64\n",
       "AGE 25-64                        int64\n",
       "AGE 65                           int64\n",
       "ILITOTAL                         int64\n",
       "NUM. OF PROVIDERS                int64\n",
       "TOTAL PATIENTS                   int64\n",
       "TOTAL SPECIMENS                  int64\n",
       "A (Subtyping not Performed)      int64\n",
       "A (2009 H1N1)                    int64\n",
       "A (H1)                           int64\n",
       "A (H3)                           int64\n",
       "B                                int64\n",
       "Bvic                             int64\n",
       "Byam                             int64\n",
       "AVG WIND SPEED MONTHLY         float64\n",
       "COOLING DAYS MONTHLY           float64\n",
       "HEATING DAYS MONTHLY           float64\n",
       "PRECIPITATION MONTHLY          float64\n",
       "AVG TEMP MONTHLY               float64\n",
       "POPULATION                       int64\n",
       "VAXEFFECTIVENESS                 int64\n",
       "VACCINERATENATIONAL            float64\n",
       "AWND                           float64\n",
       "PRCP                           float64\n",
       "TAVG                           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "class Flu_Dataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        # Read-In DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Convert 'DATE' to datetime and extract useful features\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "        df['YEAR'] = df['DATE'].dt.year\n",
    "        df['MONTH'] = df['DATE'].dt.month\n",
    "        df['WEEK'] = df['DATE'].dt.isocalendar().week\n",
    "        df.drop(\"DATE\", axis=1, inplace=True)\n",
    "\n",
    "        # Re-Order Target to Last Col\n",
    "        global cols\n",
    "        cols = df.columns.tolist()\n",
    "        cols.remove('% WEIGHTED ILI')\n",
    "        cols.append('% WEIGHTED ILI')\n",
    "        df = df[cols]\n",
    "\n",
    "        # Initialize separate scalers for features and target\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = StandardScaler()\n",
    "        \n",
    "        # Scale Features and Target separately\n",
    "        features = df.iloc[:, :-1]\n",
    "        target = df.iloc[:, -1:]\n",
    "\n",
    "        # Fit and transform features and target separately\n",
    "        scaled_features = self.feature_scaler.fit_transform(features)\n",
    "        scaled_target = self.target_scaler.fit_transform(target)\n",
    "        \n",
    "        # Convert to Torch Tensors\n",
    "        self.x = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(scaled_target, dtype=torch.float32)\n",
    "\n",
    "    # Define Mandatory length method\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    # Define Mandatory Get Item Method\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    # Split into desired train-test split\n",
    "    def split_data(self, n_test):\n",
    "        test_size = round(n_test * len(self.x))\n",
    "        train_size = len(self.x) - test_size\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length:  21\n",
      "Test Length:  6\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Class with Dataset\n",
    "flu_df = Flu_Dataset(path)\n",
    "\n",
    "# Split into 80/20 Train-Test\n",
    "train, test = flu_df.split_data(n_test=0.20)\n",
    "\n",
    "# Use DataLoader directly on the split dataset\n",
    "train = DataLoader(train, batch_size=32, shuffle=True)  # Batch size added\n",
    "test = DataLoader(test, batch_size=32, shuffle=True)    # Batch size for test set\n",
    "\n",
    "print(\"Train Length: \", len(train))\n",
    "print(\"Test Length: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Average Loss: 0.6623\n",
      "Epoch [2/1000], Average Loss: 0.2764\n",
      "Epoch [3/1000], Average Loss: 0.1741\n",
      "Epoch [4/1000], Average Loss: 0.1344\n",
      "Epoch [5/1000], Average Loss: 0.1036\n",
      "Epoch [6/1000], Average Loss: 0.0687\n",
      "Epoch [7/1000], Average Loss: 0.0630\n",
      "Epoch [8/1000], Average Loss: 0.0624\n",
      "Epoch [9/1000], Average Loss: 0.0576\n",
      "Epoch [10/1000], Average Loss: 0.0532\n",
      "Epoch [11/1000], Average Loss: 0.0416\n",
      "Epoch [12/1000], Average Loss: 0.0433\n",
      "Epoch [13/1000], Average Loss: 0.0417\n",
      "Epoch [14/1000], Average Loss: 0.0396\n",
      "Epoch [15/1000], Average Loss: 0.0285\n",
      "Epoch [16/1000], Average Loss: 0.0296\n",
      "Epoch [17/1000], Average Loss: 0.0354\n",
      "Epoch [18/1000], Average Loss: 0.0372\n",
      "Epoch [19/1000], Average Loss: 0.0308\n",
      "Epoch [20/1000], Average Loss: 0.0303\n",
      "Epoch [21/1000], Average Loss: 0.0288\n",
      "Epoch [22/1000], Average Loss: 0.0286\n",
      "Epoch [23/1000], Average Loss: 0.0304\n",
      "Epoch [24/1000], Average Loss: 0.0270\n",
      "Epoch [25/1000], Average Loss: 0.0285\n",
      "Epoch [26/1000], Average Loss: 0.0236\n",
      "Epoch [27/1000], Average Loss: 0.0247\n",
      "Epoch [28/1000], Average Loss: 0.0272\n",
      "Epoch [29/1000], Average Loss: 0.0241\n",
      "Epoch [30/1000], Average Loss: 0.0249\n",
      "Epoch [31/1000], Average Loss: 0.0222\n",
      "Epoch [32/1000], Average Loss: 0.0261\n",
      "Epoch [33/1000], Average Loss: 0.0260\n",
      "Epoch [34/1000], Average Loss: 0.0210\n",
      "Epoch [35/1000], Average Loss: 0.0199\n",
      "Epoch [36/1000], Average Loss: 0.0244\n",
      "Epoch [37/1000], Average Loss: 0.0220\n",
      "Epoch [38/1000], Average Loss: 0.0207\n",
      "Epoch [39/1000], Average Loss: 0.0247\n",
      "Epoch [40/1000], Average Loss: 0.0233\n",
      "Epoch [41/1000], Average Loss: 0.0214\n",
      "Epoch [42/1000], Average Loss: 0.0208\n",
      "Epoch [43/1000], Average Loss: 0.0185\n",
      "Epoch [44/1000], Average Loss: 0.0238\n",
      "Epoch [45/1000], Average Loss: 0.0192\n",
      "Epoch [46/1000], Average Loss: 0.0193\n",
      "Epoch [47/1000], Average Loss: 0.0205\n",
      "Epoch [48/1000], Average Loss: 0.0200\n",
      "Epoch [49/1000], Average Loss: 0.0169\n",
      "Epoch [50/1000], Average Loss: 0.0186\n",
      "Epoch [51/1000], Average Loss: 0.0245\n",
      "Epoch [52/1000], Average Loss: 0.0208\n",
      "Epoch [53/1000], Average Loss: 0.0212\n",
      "Epoch [54/1000], Average Loss: 0.0182\n",
      "Epoch [55/1000], Average Loss: 0.0210\n",
      "Epoch [56/1000], Average Loss: 0.0225\n",
      "Epoch [57/1000], Average Loss: 0.0175\n",
      "Epoch [58/1000], Average Loss: 0.0156\n",
      "Epoch [59/1000], Average Loss: 0.0199\n",
      "Epoch [60/1000], Average Loss: 0.0155\n",
      "Epoch [61/1000], Average Loss: 0.0147\n",
      "Epoch [62/1000], Average Loss: 0.0160\n",
      "Epoch [63/1000], Average Loss: 0.0176\n",
      "Epoch [64/1000], Average Loss: 0.0172\n",
      "Epoch [65/1000], Average Loss: 0.0187\n",
      "Epoch [66/1000], Average Loss: 0.0139\n",
      "Epoch [67/1000], Average Loss: 0.0166\n",
      "Epoch [68/1000], Average Loss: 0.0262\n",
      "Epoch [69/1000], Average Loss: 0.0181\n",
      "Epoch [70/1000], Average Loss: 0.0161\n",
      "Epoch [71/1000], Average Loss: 0.0199\n",
      "Epoch [72/1000], Average Loss: 0.0153\n",
      "Epoch [73/1000], Average Loss: 0.0163\n",
      "Epoch [74/1000], Average Loss: 0.0133\n",
      "Epoch [75/1000], Average Loss: 0.0134\n",
      "Epoch [76/1000], Average Loss: 0.0163\n",
      "Epoch [77/1000], Average Loss: 0.0196\n",
      "Epoch [78/1000], Average Loss: 0.0192\n",
      "Epoch [79/1000], Average Loss: 0.0197\n",
      "Epoch [80/1000], Average Loss: 0.0188\n",
      "Epoch [81/1000], Average Loss: 0.0143\n",
      "Epoch [82/1000], Average Loss: 0.0160\n",
      "Epoch [83/1000], Average Loss: 0.0136\n",
      "Epoch [84/1000], Average Loss: 0.0132\n",
      "Epoch [85/1000], Average Loss: 0.0119\n",
      "Epoch [86/1000], Average Loss: 0.0154\n",
      "Epoch [87/1000], Average Loss: 0.0149\n",
      "Epoch [88/1000], Average Loss: 0.0152\n",
      "Epoch [89/1000], Average Loss: 0.0154\n",
      "Epoch [90/1000], Average Loss: 0.0126\n",
      "Epoch [91/1000], Average Loss: 0.0143\n",
      "Epoch [92/1000], Average Loss: 0.0151\n",
      "Epoch [93/1000], Average Loss: 0.0163\n",
      "Epoch [94/1000], Average Loss: 0.0140\n",
      "Epoch [95/1000], Average Loss: 0.0157\n",
      "Epoch [96/1000], Average Loss: 0.0142\n",
      "Epoch [97/1000], Average Loss: 0.0151\n",
      "Epoch [98/1000], Average Loss: 0.0157\n",
      "Epoch [99/1000], Average Loss: 0.0128\n",
      "Epoch [100/1000], Average Loss: 0.0194\n",
      "Epoch [101/1000], Average Loss: 0.0142\n",
      "Epoch [102/1000], Average Loss: 0.0142\n",
      "Epoch [103/1000], Average Loss: 0.0145\n",
      "Epoch [104/1000], Average Loss: 0.0151\n",
      "Epoch [105/1000], Average Loss: 0.0158\n",
      "Epoch [106/1000], Average Loss: 0.0126\n",
      "Epoch [107/1000], Average Loss: 0.0127\n",
      "Epoch [108/1000], Average Loss: 0.0143\n",
      "Epoch [109/1000], Average Loss: 0.0132\n",
      "Epoch [110/1000], Average Loss: 0.0124\n",
      "Epoch [111/1000], Average Loss: 0.0159\n",
      "Epoch [112/1000], Average Loss: 0.0136\n",
      "Epoch [113/1000], Average Loss: 0.0149\n",
      "Epoch [114/1000], Average Loss: 0.0160\n",
      "Epoch [115/1000], Average Loss: 0.0127\n",
      "Epoch [116/1000], Average Loss: 0.0118\n",
      "Epoch [117/1000], Average Loss: 0.0129\n",
      "Epoch [118/1000], Average Loss: 0.0126\n",
      "Epoch [119/1000], Average Loss: 0.0150\n",
      "Epoch [120/1000], Average Loss: 0.0132\n",
      "Epoch [121/1000], Average Loss: 0.0137\n",
      "Epoch [122/1000], Average Loss: 0.0135\n",
      "Epoch [123/1000], Average Loss: 0.0151\n",
      "Epoch [124/1000], Average Loss: 0.0146\n",
      "Epoch [125/1000], Average Loss: 0.0134\n",
      "Epoch [126/1000], Average Loss: 0.0119\n",
      "Epoch [127/1000], Average Loss: 0.0138\n",
      "Epoch [128/1000], Average Loss: 0.0152\n",
      "Epoch [129/1000], Average Loss: 0.0137\n",
      "Epoch [130/1000], Average Loss: 0.0132\n",
      "Epoch [131/1000], Average Loss: 0.0126\n",
      "Epoch [132/1000], Average Loss: 0.0113\n",
      "Epoch [133/1000], Average Loss: 0.0131\n",
      "Epoch [134/1000], Average Loss: 0.0114\n",
      "Epoch [135/1000], Average Loss: 0.0181\n",
      "Epoch [136/1000], Average Loss: 0.0117\n",
      "Epoch [137/1000], Average Loss: 0.0123\n",
      "Epoch [138/1000], Average Loss: 0.0125\n",
      "Epoch [139/1000], Average Loss: 0.0135\n",
      "Epoch [140/1000], Average Loss: 0.0099\n",
      "Epoch [141/1000], Average Loss: 0.0127\n",
      "Epoch [142/1000], Average Loss: 0.0142\n",
      "Epoch [143/1000], Average Loss: 0.0128\n",
      "Epoch [144/1000], Average Loss: 0.0125\n",
      "Epoch [145/1000], Average Loss: 0.0120\n",
      "Epoch [146/1000], Average Loss: 0.0119\n",
      "Epoch [147/1000], Average Loss: 0.0121\n",
      "Epoch [148/1000], Average Loss: 0.0129\n",
      "Epoch [149/1000], Average Loss: 0.0109\n",
      "Epoch [150/1000], Average Loss: 0.0172\n",
      "Epoch [151/1000], Average Loss: 0.0144\n",
      "Epoch [152/1000], Average Loss: 0.0120\n",
      "Epoch [153/1000], Average Loss: 0.0122\n",
      "Epoch [154/1000], Average Loss: 0.0133\n",
      "Epoch [155/1000], Average Loss: 0.0109\n",
      "Epoch [156/1000], Average Loss: 0.0111\n",
      "Epoch [157/1000], Average Loss: 0.0128\n",
      "Epoch [158/1000], Average Loss: 0.0120\n",
      "Epoch [159/1000], Average Loss: 0.0137\n",
      "Epoch [160/1000], Average Loss: 0.0144\n",
      "Epoch [161/1000], Average Loss: 0.0133\n",
      "Epoch [162/1000], Average Loss: 0.0166\n",
      "Epoch [163/1000], Average Loss: 0.0111\n",
      "Epoch [164/1000], Average Loss: 0.0097\n",
      "Epoch [165/1000], Average Loss: 0.0109\n",
      "Epoch [166/1000], Average Loss: 0.0116\n",
      "Epoch [167/1000], Average Loss: 0.0122\n",
      "Epoch [168/1000], Average Loss: 0.0109\n",
      "Epoch [169/1000], Average Loss: 0.0124\n",
      "Epoch [170/1000], Average Loss: 0.0124\n",
      "Epoch [171/1000], Average Loss: 0.0101\n",
      "Epoch [172/1000], Average Loss: 0.0102\n",
      "Epoch [173/1000], Average Loss: 0.0102\n",
      "Epoch [174/1000], Average Loss: 0.0112\n",
      "Epoch [175/1000], Average Loss: 0.0128\n",
      "Epoch [176/1000], Average Loss: 0.0107\n",
      "Epoch [177/1000], Average Loss: 0.0111\n",
      "Epoch [178/1000], Average Loss: 0.0095\n",
      "Epoch [179/1000], Average Loss: 0.0118\n",
      "Epoch [180/1000], Average Loss: 0.0134\n",
      "Epoch [181/1000], Average Loss: 0.0101\n",
      "Epoch [182/1000], Average Loss: 0.0093\n",
      "Epoch [183/1000], Average Loss: 0.0090\n",
      "Epoch [184/1000], Average Loss: 0.0116\n",
      "Epoch [185/1000], Average Loss: 0.0092\n",
      "Epoch [186/1000], Average Loss: 0.0107\n",
      "Epoch [187/1000], Average Loss: 0.0099\n",
      "Epoch [188/1000], Average Loss: 0.0101\n",
      "Epoch [189/1000], Average Loss: 0.0144\n",
      "Epoch [190/1000], Average Loss: 0.0105\n",
      "Epoch [191/1000], Average Loss: 0.0116\n",
      "Epoch [192/1000], Average Loss: 0.0095\n",
      "Epoch [193/1000], Average Loss: 0.0106\n",
      "Epoch [194/1000], Average Loss: 0.0114\n",
      "Epoch [195/1000], Average Loss: 0.0107\n",
      "Epoch [196/1000], Average Loss: 0.0103\n",
      "Epoch [197/1000], Average Loss: 0.0106\n",
      "Epoch [198/1000], Average Loss: 0.0121\n",
      "Epoch [199/1000], Average Loss: 0.0119\n",
      "Epoch [200/1000], Average Loss: 0.0089\n",
      "Epoch [201/1000], Average Loss: 0.0108\n",
      "Epoch [202/1000], Average Loss: 0.0123\n",
      "Epoch [203/1000], Average Loss: 0.0102\n",
      "Epoch [204/1000], Average Loss: 0.0114\n",
      "Epoch [205/1000], Average Loss: 0.0103\n",
      "Epoch [206/1000], Average Loss: 0.0113\n",
      "Epoch [207/1000], Average Loss: 0.0106\n",
      "Epoch [208/1000], Average Loss: 0.0096\n",
      "Epoch [209/1000], Average Loss: 0.0111\n",
      "Epoch [210/1000], Average Loss: 0.0114\n",
      "Epoch [211/1000], Average Loss: 0.0099\n",
      "Epoch [212/1000], Average Loss: 0.0106\n",
      "Epoch [213/1000], Average Loss: 0.0096\n",
      "Epoch [214/1000], Average Loss: 0.0104\n",
      "Epoch [215/1000], Average Loss: 0.0116\n",
      "Epoch [216/1000], Average Loss: 0.0096\n",
      "Epoch [217/1000], Average Loss: 0.0096\n",
      "Epoch [218/1000], Average Loss: 0.0105\n",
      "Epoch [219/1000], Average Loss: 0.0111\n",
      "Epoch [220/1000], Average Loss: 0.0117\n",
      "Epoch [221/1000], Average Loss: 0.0088\n",
      "Epoch [222/1000], Average Loss: 0.0112\n",
      "Epoch [223/1000], Average Loss: 0.0116\n",
      "Epoch [224/1000], Average Loss: 0.0096\n",
      "Epoch [225/1000], Average Loss: 0.0126\n",
      "Epoch [226/1000], Average Loss: 0.0096\n",
      "Epoch [227/1000], Average Loss: 0.0101\n",
      "Epoch [228/1000], Average Loss: 0.0109\n",
      "Epoch [229/1000], Average Loss: 0.0096\n",
      "Epoch [230/1000], Average Loss: 0.0094\n",
      "Epoch [231/1000], Average Loss: 0.0103\n",
      "Epoch [232/1000], Average Loss: 0.0095\n",
      "Epoch [233/1000], Average Loss: 0.0091\n",
      "Epoch [234/1000], Average Loss: 0.0108\n",
      "Epoch [235/1000], Average Loss: 0.0111\n",
      "Epoch [236/1000], Average Loss: 0.0084\n",
      "Epoch [237/1000], Average Loss: 0.0104\n",
      "Epoch [238/1000], Average Loss: 0.0095\n",
      "Epoch [239/1000], Average Loss: 0.0080\n",
      "Epoch [240/1000], Average Loss: 0.0090\n",
      "Epoch [241/1000], Average Loss: 0.0089\n",
      "Epoch [242/1000], Average Loss: 0.0105\n",
      "Epoch [243/1000], Average Loss: 0.0081\n",
      "Epoch [244/1000], Average Loss: 0.0107\n",
      "Epoch [245/1000], Average Loss: 0.0088\n",
      "Epoch [246/1000], Average Loss: 0.0105\n",
      "Epoch [247/1000], Average Loss: 0.0121\n",
      "Epoch [248/1000], Average Loss: 0.0088\n",
      "Epoch [249/1000], Average Loss: 0.0092\n",
      "Epoch [250/1000], Average Loss: 0.0117\n",
      "Epoch [251/1000], Average Loss: 0.0107\n",
      "Epoch [252/1000], Average Loss: 0.0089\n",
      "Epoch [253/1000], Average Loss: 0.0083\n",
      "Epoch [254/1000], Average Loss: 0.0079\n",
      "Epoch [255/1000], Average Loss: 0.0086\n",
      "Epoch [256/1000], Average Loss: 0.0091\n",
      "Epoch [257/1000], Average Loss: 0.0100\n",
      "Epoch [258/1000], Average Loss: 0.0092\n",
      "Epoch [259/1000], Average Loss: 0.0096\n",
      "Epoch [260/1000], Average Loss: 0.0087\n",
      "Epoch [261/1000], Average Loss: 0.0093\n",
      "Epoch [262/1000], Average Loss: 0.0087\n",
      "Epoch [263/1000], Average Loss: 0.0077\n",
      "Epoch [264/1000], Average Loss: 0.0086\n",
      "Epoch [265/1000], Average Loss: 0.0096\n",
      "Epoch [266/1000], Average Loss: 0.0092\n",
      "Epoch [267/1000], Average Loss: 0.0092\n",
      "Epoch [268/1000], Average Loss: 0.0111\n",
      "Epoch [269/1000], Average Loss: 0.0092\n",
      "Epoch [270/1000], Average Loss: 0.0136\n",
      "Epoch [271/1000], Average Loss: 0.0107\n",
      "Epoch [272/1000], Average Loss: 0.0083\n",
      "Epoch [273/1000], Average Loss: 0.0100\n",
      "Epoch [274/1000], Average Loss: 0.0115\n",
      "Epoch [275/1000], Average Loss: 0.0123\n",
      "Epoch [276/1000], Average Loss: 0.0105\n",
      "Epoch [277/1000], Average Loss: 0.0104\n",
      "Epoch [278/1000], Average Loss: 0.0104\n",
      "Epoch [279/1000], Average Loss: 0.0106\n",
      "Epoch [280/1000], Average Loss: 0.0097\n",
      "Epoch [281/1000], Average Loss: 0.0109\n",
      "Epoch [282/1000], Average Loss: 0.0117\n",
      "Epoch [283/1000], Average Loss: 0.0095\n",
      "Epoch [284/1000], Average Loss: 0.0097\n",
      "Epoch [285/1000], Average Loss: 0.0081\n",
      "Epoch [286/1000], Average Loss: 0.0097\n",
      "Epoch [287/1000], Average Loss: 0.0091\n",
      "Epoch [288/1000], Average Loss: 0.0092\n",
      "Epoch [289/1000], Average Loss: 0.0116\n",
      "Epoch [290/1000], Average Loss: 0.0078\n",
      "Epoch [291/1000], Average Loss: 0.0097\n",
      "Epoch [292/1000], Average Loss: 0.0086\n",
      "Epoch [293/1000], Average Loss: 0.0099\n",
      "Epoch [294/1000], Average Loss: 0.0098\n",
      "Epoch [295/1000], Average Loss: 0.0082\n",
      "Epoch [296/1000], Average Loss: 0.0091\n",
      "Epoch [297/1000], Average Loss: 0.0115\n",
      "Epoch [298/1000], Average Loss: 0.0093\n",
      "Epoch [299/1000], Average Loss: 0.0096\n",
      "Epoch [300/1000], Average Loss: 0.0092\n",
      "Epoch [301/1000], Average Loss: 0.0098\n",
      "Epoch [302/1000], Average Loss: 0.0089\n",
      "Epoch [303/1000], Average Loss: 0.0079\n",
      "Epoch [304/1000], Average Loss: 0.0078\n",
      "Epoch [305/1000], Average Loss: 0.0113\n",
      "Epoch [306/1000], Average Loss: 0.0082\n",
      "Epoch [307/1000], Average Loss: 0.0099\n",
      "Epoch [308/1000], Average Loss: 0.0116\n",
      "Epoch [309/1000], Average Loss: 0.0103\n",
      "Epoch [310/1000], Average Loss: 0.0119\n",
      "Epoch [311/1000], Average Loss: 0.0092\n",
      "Epoch [312/1000], Average Loss: 0.0095\n",
      "Epoch [313/1000], Average Loss: 0.0110\n",
      "Epoch [314/1000], Average Loss: 0.0097\n",
      "Epoch [315/1000], Average Loss: 0.0086\n",
      "Epoch [316/1000], Average Loss: 0.0074\n",
      "Epoch [317/1000], Average Loss: 0.0091\n",
      "Epoch [318/1000], Average Loss: 0.0087\n",
      "Epoch [319/1000], Average Loss: 0.0090\n",
      "Epoch [320/1000], Average Loss: 0.0081\n",
      "Epoch [321/1000], Average Loss: 0.0088\n",
      "Epoch [322/1000], Average Loss: 0.0079\n",
      "Epoch [323/1000], Average Loss: 0.0107\n",
      "Epoch [324/1000], Average Loss: 0.0085\n",
      "Epoch [325/1000], Average Loss: 0.0094\n",
      "Epoch [326/1000], Average Loss: 0.0083\n",
      "Epoch [327/1000], Average Loss: 0.0083\n",
      "Epoch [328/1000], Average Loss: 0.0095\n",
      "Epoch [329/1000], Average Loss: 0.0108\n",
      "Epoch [330/1000], Average Loss: 0.0089\n",
      "Epoch [331/1000], Average Loss: 0.0106\n",
      "Epoch [332/1000], Average Loss: 0.0094\n",
      "Epoch [333/1000], Average Loss: 0.0100\n",
      "Epoch [334/1000], Average Loss: 0.0108\n",
      "Epoch [335/1000], Average Loss: 0.0084\n",
      "Epoch [336/1000], Average Loss: 0.0090\n",
      "Epoch [337/1000], Average Loss: 0.0095\n",
      "Epoch [338/1000], Average Loss: 0.0108\n",
      "Epoch [339/1000], Average Loss: 0.0131\n",
      "Epoch [340/1000], Average Loss: 0.0087\n",
      "Epoch [341/1000], Average Loss: 0.0095\n",
      "Epoch [342/1000], Average Loss: 0.0106\n",
      "Epoch [343/1000], Average Loss: 0.0121\n",
      "Epoch [344/1000], Average Loss: 0.0097\n",
      "Epoch [345/1000], Average Loss: 0.0077\n",
      "Epoch [346/1000], Average Loss: 0.0074\n",
      "Epoch [347/1000], Average Loss: 0.0107\n",
      "Epoch [348/1000], Average Loss: 0.0076\n",
      "Epoch [349/1000], Average Loss: 0.0099\n",
      "Epoch [350/1000], Average Loss: 0.0116\n",
      "Epoch [351/1000], Average Loss: 0.0122\n",
      "Epoch [352/1000], Average Loss: 0.0078\n",
      "Epoch [353/1000], Average Loss: 0.0086\n",
      "Epoch [354/1000], Average Loss: 0.0087\n",
      "Epoch [355/1000], Average Loss: 0.0067\n",
      "Epoch [356/1000], Average Loss: 0.0091\n",
      "Epoch [357/1000], Average Loss: 0.0067\n",
      "Epoch [358/1000], Average Loss: 0.0066\n",
      "Epoch [359/1000], Average Loss: 0.0100\n",
      "Epoch [360/1000], Average Loss: 0.0078\n",
      "Epoch [361/1000], Average Loss: 0.0075\n",
      "Epoch [362/1000], Average Loss: 0.0064\n",
      "Epoch [363/1000], Average Loss: 0.0075\n",
      "Epoch [364/1000], Average Loss: 0.0073\n",
      "Epoch [365/1000], Average Loss: 0.0097\n",
      "Epoch [366/1000], Average Loss: 0.0111\n",
      "Epoch [367/1000], Average Loss: 0.0067\n",
      "Epoch [368/1000], Average Loss: 0.0114\n",
      "Epoch [369/1000], Average Loss: 0.0086\n",
      "Epoch [370/1000], Average Loss: 0.0083\n",
      "Epoch [371/1000], Average Loss: 0.0076\n",
      "Epoch [372/1000], Average Loss: 0.0087\n",
      "Epoch [373/1000], Average Loss: 0.0072\n",
      "Epoch [374/1000], Average Loss: 0.0091\n",
      "Epoch [375/1000], Average Loss: 0.0094\n",
      "Epoch [376/1000], Average Loss: 0.0073\n",
      "Epoch [377/1000], Average Loss: 0.0088\n",
      "Epoch [378/1000], Average Loss: 0.0075\n",
      "Epoch [379/1000], Average Loss: 0.0076\n",
      "Epoch [380/1000], Average Loss: 0.0112\n",
      "Epoch [381/1000], Average Loss: 0.0094\n",
      "Epoch [382/1000], Average Loss: 0.0079\n",
      "Epoch [383/1000], Average Loss: 0.0072\n",
      "Epoch [384/1000], Average Loss: 0.0079\n",
      "Epoch [385/1000], Average Loss: 0.0071\n",
      "Epoch [386/1000], Average Loss: 0.0081\n",
      "Epoch [387/1000], Average Loss: 0.0088\n",
      "Epoch [388/1000], Average Loss: 0.0080\n",
      "Epoch [389/1000], Average Loss: 0.0077\n",
      "Epoch [390/1000], Average Loss: 0.0086\n",
      "Epoch [391/1000], Average Loss: 0.0092\n",
      "Epoch [392/1000], Average Loss: 0.0081\n",
      "Epoch [393/1000], Average Loss: 0.0094\n",
      "Epoch [394/1000], Average Loss: 0.0092\n",
      "Epoch [395/1000], Average Loss: 0.0065\n",
      "Epoch [396/1000], Average Loss: 0.0089\n",
      "Epoch [397/1000], Average Loss: 0.0078\n",
      "Epoch [398/1000], Average Loss: 0.0074\n",
      "Epoch [399/1000], Average Loss: 0.0066\n",
      "Epoch [400/1000], Average Loss: 0.0083\n",
      "Epoch [401/1000], Average Loss: 0.0084\n",
      "Epoch [402/1000], Average Loss: 0.0080\n",
      "Epoch [403/1000], Average Loss: 0.0090\n",
      "Epoch [404/1000], Average Loss: 0.0085\n",
      "Epoch [405/1000], Average Loss: 0.0084\n",
      "Epoch [406/1000], Average Loss: 0.0078\n",
      "Epoch [407/1000], Average Loss: 0.0073\n",
      "Epoch [408/1000], Average Loss: 0.0090\n",
      "Epoch [409/1000], Average Loss: 0.0075\n",
      "Epoch [410/1000], Average Loss: 0.0094\n",
      "Epoch [411/1000], Average Loss: 0.0074\n",
      "Epoch [412/1000], Average Loss: 0.0086\n",
      "Epoch [413/1000], Average Loss: 0.0083\n",
      "Epoch [414/1000], Average Loss: 0.0072\n",
      "Epoch [415/1000], Average Loss: 0.0075\n",
      "Epoch [416/1000], Average Loss: 0.0069\n",
      "Epoch [417/1000], Average Loss: 0.0063\n",
      "Epoch [418/1000], Average Loss: 0.0073\n",
      "Epoch [419/1000], Average Loss: 0.0054\n",
      "Epoch [420/1000], Average Loss: 0.0087\n",
      "Epoch [421/1000], Average Loss: 0.0090\n",
      "Epoch [422/1000], Average Loss: 0.0088\n",
      "Epoch [423/1000], Average Loss: 0.0077\n",
      "Epoch [424/1000], Average Loss: 0.0071\n",
      "Epoch [425/1000], Average Loss: 0.0094\n",
      "Epoch [426/1000], Average Loss: 0.0080\n",
      "Epoch [427/1000], Average Loss: 0.0074\n",
      "Epoch [428/1000], Average Loss: 0.0074\n",
      "Epoch [429/1000], Average Loss: 0.0078\n",
      "Epoch [430/1000], Average Loss: 0.0076\n",
      "Epoch [431/1000], Average Loss: 0.0079\n",
      "Epoch [432/1000], Average Loss: 0.0080\n",
      "Epoch [433/1000], Average Loss: 0.0081\n",
      "Epoch [434/1000], Average Loss: 0.0078\n",
      "Epoch [435/1000], Average Loss: 0.0081\n",
      "Epoch [436/1000], Average Loss: 0.0076\n",
      "Epoch [437/1000], Average Loss: 0.0074\n",
      "Epoch [438/1000], Average Loss: 0.0081\n",
      "Epoch [439/1000], Average Loss: 0.0070\n",
      "Epoch [440/1000], Average Loss: 0.0057\n",
      "Epoch [441/1000], Average Loss: 0.0078\n",
      "Epoch [442/1000], Average Loss: 0.0079\n",
      "Epoch [443/1000], Average Loss: 0.0072\n",
      "Epoch [444/1000], Average Loss: 0.0074\n",
      "Epoch [445/1000], Average Loss: 0.0076\n",
      "Epoch [446/1000], Average Loss: 0.0072\n",
      "Epoch [447/1000], Average Loss: 0.0072\n",
      "Epoch [448/1000], Average Loss: 0.0078\n",
      "Epoch [449/1000], Average Loss: 0.0096\n",
      "Epoch [450/1000], Average Loss: 0.0073\n",
      "Epoch [451/1000], Average Loss: 0.0070\n",
      "Epoch [452/1000], Average Loss: 0.0070\n",
      "Epoch [453/1000], Average Loss: 0.0068\n",
      "Epoch [454/1000], Average Loss: 0.0084\n",
      "Epoch [455/1000], Average Loss: 0.0089\n",
      "Epoch [456/1000], Average Loss: 0.0099\n",
      "Epoch [457/1000], Average Loss: 0.0071\n",
      "Epoch [458/1000], Average Loss: 0.0071\n",
      "Epoch [459/1000], Average Loss: 0.0079\n",
      "Epoch [460/1000], Average Loss: 0.0090\n",
      "Epoch [461/1000], Average Loss: 0.0093\n",
      "Epoch [462/1000], Average Loss: 0.0074\n",
      "Epoch [463/1000], Average Loss: 0.0072\n",
      "Epoch [464/1000], Average Loss: 0.0081\n",
      "Epoch [465/1000], Average Loss: 0.0075\n",
      "Epoch [466/1000], Average Loss: 0.0081\n",
      "Epoch [467/1000], Average Loss: 0.0091\n",
      "Epoch [468/1000], Average Loss: 0.0083\n",
      "Epoch [469/1000], Average Loss: 0.0085\n",
      "Epoch [470/1000], Average Loss: 0.0067\n",
      "Epoch [471/1000], Average Loss: 0.0062\n",
      "Epoch [472/1000], Average Loss: 0.0070\n",
      "Epoch [473/1000], Average Loss: 0.0086\n",
      "Epoch [474/1000], Average Loss: 0.0066\n",
      "Epoch [475/1000], Average Loss: 0.0065\n",
      "Epoch [476/1000], Average Loss: 0.0070\n",
      "Epoch [477/1000], Average Loss: 0.0087\n",
      "Epoch [478/1000], Average Loss: 0.0074\n",
      "Epoch [479/1000], Average Loss: 0.0086\n",
      "Epoch [480/1000], Average Loss: 0.0072\n",
      "Epoch [481/1000], Average Loss: 0.0079\n",
      "Epoch [482/1000], Average Loss: 0.0064\n",
      "Epoch [483/1000], Average Loss: 0.0064\n",
      "Epoch [484/1000], Average Loss: 0.0083\n",
      "Epoch [485/1000], Average Loss: 0.0079\n",
      "Epoch [486/1000], Average Loss: 0.0074\n",
      "Epoch [487/1000], Average Loss: 0.0076\n",
      "Epoch [488/1000], Average Loss: 0.0070\n",
      "Epoch [489/1000], Average Loss: 0.0070\n",
      "Epoch [490/1000], Average Loss: 0.0059\n",
      "Epoch [491/1000], Average Loss: 0.0060\n",
      "Epoch [492/1000], Average Loss: 0.0073\n",
      "Epoch [493/1000], Average Loss: 0.0074\n",
      "Epoch [494/1000], Average Loss: 0.0068\n",
      "Epoch [495/1000], Average Loss: 0.0090\n",
      "Epoch [496/1000], Average Loss: 0.0079\n",
      "Epoch [497/1000], Average Loss: 0.0058\n",
      "Epoch [498/1000], Average Loss: 0.0078\n",
      "Epoch [499/1000], Average Loss: 0.0076\n",
      "Epoch [500/1000], Average Loss: 0.0072\n",
      "Epoch [501/1000], Average Loss: 0.0089\n",
      "Epoch [502/1000], Average Loss: 0.0076\n",
      "Epoch [503/1000], Average Loss: 0.0059\n",
      "Epoch [504/1000], Average Loss: 0.0060\n",
      "Epoch [505/1000], Average Loss: 0.0072\n",
      "Epoch [506/1000], Average Loss: 0.0064\n",
      "Epoch [507/1000], Average Loss: 0.0052\n",
      "Epoch [508/1000], Average Loss: 0.0082\n",
      "Epoch [509/1000], Average Loss: 0.0058\n",
      "Epoch [510/1000], Average Loss: 0.0063\n",
      "Epoch [511/1000], Average Loss: 0.0066\n",
      "Epoch [512/1000], Average Loss: 0.0071\n",
      "Epoch [513/1000], Average Loss: 0.0059\n",
      "Epoch [514/1000], Average Loss: 0.0058\n",
      "Epoch [515/1000], Average Loss: 0.0062\n",
      "Epoch [516/1000], Average Loss: 0.0083\n",
      "Epoch [517/1000], Average Loss: 0.0068\n",
      "Epoch [518/1000], Average Loss: 0.0066\n",
      "Epoch [519/1000], Average Loss: 0.0069\n",
      "Epoch [520/1000], Average Loss: 0.0084\n",
      "Epoch [521/1000], Average Loss: 0.0059\n",
      "Epoch [522/1000], Average Loss: 0.0078\n",
      "Epoch [523/1000], Average Loss: 0.0087\n",
      "Epoch [524/1000], Average Loss: 0.0072\n",
      "Epoch [525/1000], Average Loss: 0.0083\n",
      "Epoch [526/1000], Average Loss: 0.0076\n",
      "Epoch [527/1000], Average Loss: 0.0057\n",
      "Epoch [528/1000], Average Loss: 0.0073\n",
      "Epoch [529/1000], Average Loss: 0.0070\n",
      "Epoch [530/1000], Average Loss: 0.0076\n",
      "Epoch [531/1000], Average Loss: 0.0079\n",
      "Epoch [532/1000], Average Loss: 0.0074\n",
      "Epoch [533/1000], Average Loss: 0.0070\n",
      "Epoch [534/1000], Average Loss: 0.0063\n",
      "Epoch [535/1000], Average Loss: 0.0070\n",
      "Epoch [536/1000], Average Loss: 0.0070\n",
      "Epoch [537/1000], Average Loss: 0.0072\n",
      "Epoch [538/1000], Average Loss: 0.0080\n",
      "Epoch [539/1000], Average Loss: 0.0061\n",
      "Epoch [540/1000], Average Loss: 0.0075\n",
      "Epoch [541/1000], Average Loss: 0.0072\n",
      "Epoch [542/1000], Average Loss: 0.0067\n",
      "Epoch [543/1000], Average Loss: 0.0078\n",
      "Epoch [544/1000], Average Loss: 0.0070\n",
      "Epoch [545/1000], Average Loss: 0.0079\n",
      "Epoch [546/1000], Average Loss: 0.0063\n",
      "Epoch [547/1000], Average Loss: 0.0072\n",
      "Epoch [548/1000], Average Loss: 0.0067\n",
      "Epoch [549/1000], Average Loss: 0.0064\n",
      "Epoch [550/1000], Average Loss: 0.0066\n",
      "Epoch [551/1000], Average Loss: 0.0079\n",
      "Epoch [552/1000], Average Loss: 0.0072\n",
      "Epoch [553/1000], Average Loss: 0.0081\n",
      "Epoch [554/1000], Average Loss: 0.0059\n",
      "Epoch [555/1000], Average Loss: 0.0085\n",
      "Epoch [556/1000], Average Loss: 0.0085\n",
      "Epoch [557/1000], Average Loss: 0.0069\n",
      "Epoch [558/1000], Average Loss: 0.0067\n",
      "Epoch [559/1000], Average Loss: 0.0058\n",
      "Epoch [560/1000], Average Loss: 0.0073\n",
      "Epoch [561/1000], Average Loss: 0.0072\n",
      "Epoch [562/1000], Average Loss: 0.0075\n",
      "Epoch [563/1000], Average Loss: 0.0061\n",
      "Epoch [564/1000], Average Loss: 0.0063\n",
      "Epoch [565/1000], Average Loss: 0.0059\n",
      "Epoch [566/1000], Average Loss: 0.0057\n",
      "Epoch [567/1000], Average Loss: 0.0069\n",
      "Epoch [568/1000], Average Loss: 0.0056\n",
      "Epoch [569/1000], Average Loss: 0.0071\n",
      "Epoch [570/1000], Average Loss: 0.0057\n",
      "Epoch [571/1000], Average Loss: 0.0072\n",
      "Epoch [572/1000], Average Loss: 0.0085\n",
      "Epoch [573/1000], Average Loss: 0.0056\n",
      "Epoch [574/1000], Average Loss: 0.0062\n",
      "Epoch [575/1000], Average Loss: 0.0066\n",
      "Epoch [576/1000], Average Loss: 0.0062\n",
      "Epoch [577/1000], Average Loss: 0.0064\n",
      "Epoch [578/1000], Average Loss: 0.0097\n",
      "Epoch [579/1000], Average Loss: 0.0089\n",
      "Epoch [580/1000], Average Loss: 0.0065\n",
      "Epoch [581/1000], Average Loss: 0.0060\n",
      "Epoch [582/1000], Average Loss: 0.0074\n",
      "Epoch [583/1000], Average Loss: 0.0057\n",
      "Epoch [584/1000], Average Loss: 0.0054\n",
      "Epoch [585/1000], Average Loss: 0.0061\n",
      "Epoch [586/1000], Average Loss: 0.0062\n",
      "Epoch [587/1000], Average Loss: 0.0063\n",
      "Epoch [588/1000], Average Loss: 0.0064\n",
      "Epoch [589/1000], Average Loss: 0.0111\n",
      "Epoch [590/1000], Average Loss: 0.0084\n",
      "Epoch [591/1000], Average Loss: 0.0071\n",
      "Epoch [592/1000], Average Loss: 0.0060\n",
      "Epoch [593/1000], Average Loss: 0.0099\n",
      "Epoch [594/1000], Average Loss: 0.0078\n",
      "Epoch [595/1000], Average Loss: 0.0065\n",
      "Epoch [596/1000], Average Loss: 0.0063\n",
      "Epoch [597/1000], Average Loss: 0.0102\n",
      "Epoch [598/1000], Average Loss: 0.0081\n",
      "Epoch [599/1000], Average Loss: 0.0080\n",
      "Epoch [600/1000], Average Loss: 0.0072\n",
      "Epoch [601/1000], Average Loss: 0.0073\n",
      "Epoch [602/1000], Average Loss: 0.0091\n",
      "Epoch [603/1000], Average Loss: 0.0084\n",
      "Epoch [604/1000], Average Loss: 0.0060\n",
      "Epoch [605/1000], Average Loss: 0.0060\n",
      "Epoch [606/1000], Average Loss: 0.0076\n",
      "Epoch [607/1000], Average Loss: 0.0064\n",
      "Epoch [608/1000], Average Loss: 0.0070\n",
      "Epoch [609/1000], Average Loss: 0.0087\n",
      "Epoch [610/1000], Average Loss: 0.0063\n",
      "Epoch [611/1000], Average Loss: 0.0077\n",
      "Epoch [612/1000], Average Loss: 0.0061\n",
      "Epoch [613/1000], Average Loss: 0.0057\n",
      "Epoch [614/1000], Average Loss: 0.0055\n",
      "Epoch [615/1000], Average Loss: 0.0074\n",
      "Epoch [616/1000], Average Loss: 0.0063\n",
      "Epoch [617/1000], Average Loss: 0.0062\n",
      "Epoch [618/1000], Average Loss: 0.0062\n",
      "Epoch [619/1000], Average Loss: 0.0070\n",
      "Epoch [620/1000], Average Loss: 0.0074\n",
      "Epoch [621/1000], Average Loss: 0.0079\n",
      "Epoch [622/1000], Average Loss: 0.0079\n",
      "Epoch [623/1000], Average Loss: 0.0067\n",
      "Epoch [624/1000], Average Loss: 0.0067\n",
      "Epoch [625/1000], Average Loss: 0.0062\n",
      "Epoch [626/1000], Average Loss: 0.0060\n",
      "Epoch [627/1000], Average Loss: 0.0063\n",
      "Epoch [628/1000], Average Loss: 0.0072\n",
      "Epoch [629/1000], Average Loss: 0.0063\n",
      "Epoch [630/1000], Average Loss: 0.0070\n",
      "Epoch [631/1000], Average Loss: 0.0058\n",
      "Epoch [632/1000], Average Loss: 0.0060\n",
      "Epoch [633/1000], Average Loss: 0.0071\n",
      "Epoch [634/1000], Average Loss: 0.0056\n",
      "Epoch [635/1000], Average Loss: 0.0053\n",
      "Epoch [636/1000], Average Loss: 0.0084\n",
      "Epoch [637/1000], Average Loss: 0.0071\n",
      "Epoch [638/1000], Average Loss: 0.0061\n",
      "Epoch [639/1000], Average Loss: 0.0060\n",
      "Epoch [640/1000], Average Loss: 0.0060\n",
      "Epoch [641/1000], Average Loss: 0.0057\n",
      "Epoch [642/1000], Average Loss: 0.0053\n",
      "Epoch [643/1000], Average Loss: 0.0059\n",
      "Epoch [644/1000], Average Loss: 0.0076\n",
      "Epoch [645/1000], Average Loss: 0.0071\n",
      "Epoch [646/1000], Average Loss: 0.0069\n",
      "Epoch [647/1000], Average Loss: 0.0066\n",
      "Epoch [648/1000], Average Loss: 0.0073\n",
      "Epoch [649/1000], Average Loss: 0.0086\n",
      "Epoch [650/1000], Average Loss: 0.0088\n",
      "Epoch [651/1000], Average Loss: 0.0076\n",
      "Epoch [652/1000], Average Loss: 0.0080\n",
      "Epoch [653/1000], Average Loss: 0.0076\n",
      "Epoch [654/1000], Average Loss: 0.0068\n",
      "Epoch [655/1000], Average Loss: 0.0060\n",
      "Epoch [656/1000], Average Loss: 0.0072\n",
      "Epoch [657/1000], Average Loss: 0.0077\n",
      "Epoch [658/1000], Average Loss: 0.0068\n",
      "Epoch [659/1000], Average Loss: 0.0067\n",
      "Epoch [660/1000], Average Loss: 0.0060\n",
      "Epoch [661/1000], Average Loss: 0.0068\n",
      "Epoch [662/1000], Average Loss: 0.0078\n",
      "Epoch [663/1000], Average Loss: 0.0058\n",
      "Epoch [664/1000], Average Loss: 0.0048\n",
      "Epoch [665/1000], Average Loss: 0.0068\n",
      "Epoch [666/1000], Average Loss: 0.0058\n",
      "Epoch [667/1000], Average Loss: 0.0053\n",
      "Epoch [668/1000], Average Loss: 0.0053\n",
      "Epoch [669/1000], Average Loss: 0.0065\n",
      "Epoch [670/1000], Average Loss: 0.0060\n",
      "Epoch [671/1000], Average Loss: 0.0055\n",
      "Epoch [672/1000], Average Loss: 0.0066\n",
      "Epoch [673/1000], Average Loss: 0.0081\n",
      "Epoch [674/1000], Average Loss: 0.0081\n",
      "Epoch [675/1000], Average Loss: 0.0060\n",
      "Epoch [676/1000], Average Loss: 0.0085\n",
      "Epoch [677/1000], Average Loss: 0.0065\n",
      "Epoch [678/1000], Average Loss: 0.0068\n",
      "Epoch [679/1000], Average Loss: 0.0069\n",
      "Epoch [680/1000], Average Loss: 0.0065\n",
      "Epoch [681/1000], Average Loss: 0.0054\n",
      "Epoch [682/1000], Average Loss: 0.0058\n",
      "Epoch [683/1000], Average Loss: 0.0050\n",
      "Epoch [684/1000], Average Loss: 0.0073\n",
      "Epoch [685/1000], Average Loss: 0.0067\n",
      "Epoch [686/1000], Average Loss: 0.0068\n",
      "Epoch [687/1000], Average Loss: 0.0057\n",
      "Epoch [688/1000], Average Loss: 0.0055\n",
      "Epoch [689/1000], Average Loss: 0.0066\n",
      "Epoch [690/1000], Average Loss: 0.0054\n",
      "Epoch [691/1000], Average Loss: 0.0057\n",
      "Epoch [692/1000], Average Loss: 0.0050\n",
      "Epoch [693/1000], Average Loss: 0.0064\n",
      "Epoch [694/1000], Average Loss: 0.0060\n",
      "Epoch [695/1000], Average Loss: 0.0063\n",
      "Epoch [696/1000], Average Loss: 0.0076\n",
      "Epoch [697/1000], Average Loss: 0.0072\n",
      "Epoch [698/1000], Average Loss: 0.0063\n",
      "Epoch [699/1000], Average Loss: 0.0065\n",
      "Epoch [700/1000], Average Loss: 0.0054\n",
      "Epoch [701/1000], Average Loss: 0.0062\n",
      "Epoch [702/1000], Average Loss: 0.0054\n",
      "Epoch [703/1000], Average Loss: 0.0074\n",
      "Epoch [704/1000], Average Loss: 0.0064\n",
      "Epoch [705/1000], Average Loss: 0.0059\n",
      "Epoch [706/1000], Average Loss: 0.0052\n",
      "Epoch [707/1000], Average Loss: 0.0069\n",
      "Epoch [708/1000], Average Loss: 0.0066\n",
      "Epoch [709/1000], Average Loss: 0.0062\n",
      "Epoch [710/1000], Average Loss: 0.0065\n",
      "Epoch [711/1000], Average Loss: 0.0048\n",
      "Epoch [712/1000], Average Loss: 0.0059\n",
      "Epoch [713/1000], Average Loss: 0.0059\n",
      "Epoch [714/1000], Average Loss: 0.0057\n",
      "Epoch [715/1000], Average Loss: 0.0075\n",
      "Epoch [716/1000], Average Loss: 0.0053\n",
      "Epoch [717/1000], Average Loss: 0.0062\n",
      "Epoch [718/1000], Average Loss: 0.0055\n",
      "Epoch [719/1000], Average Loss: 0.0063\n",
      "Epoch [720/1000], Average Loss: 0.0061\n",
      "Epoch [721/1000], Average Loss: 0.0058\n",
      "Epoch [722/1000], Average Loss: 0.0060\n",
      "Epoch [723/1000], Average Loss: 0.0052\n",
      "Epoch [724/1000], Average Loss: 0.0068\n",
      "Epoch [725/1000], Average Loss: 0.0062\n",
      "Epoch [726/1000], Average Loss: 0.0075\n",
      "Epoch [727/1000], Average Loss: 0.0072\n",
      "Epoch [728/1000], Average Loss: 0.0069\n",
      "Epoch [729/1000], Average Loss: 0.0068\n",
      "Epoch [730/1000], Average Loss: 0.0070\n",
      "Epoch [731/1000], Average Loss: 0.0061\n",
      "Epoch [732/1000], Average Loss: 0.0056\n",
      "Epoch [733/1000], Average Loss: 0.0065\n",
      "Epoch [734/1000], Average Loss: 0.0063\n",
      "Epoch [735/1000], Average Loss: 0.0069\n",
      "Epoch [736/1000], Average Loss: 0.0070\n",
      "Epoch [737/1000], Average Loss: 0.0058\n",
      "Epoch [738/1000], Average Loss: 0.0057\n",
      "Epoch [739/1000], Average Loss: 0.0066\n",
      "Epoch [740/1000], Average Loss: 0.0072\n",
      "Epoch [741/1000], Average Loss: 0.0073\n",
      "Epoch [742/1000], Average Loss: 0.0085\n",
      "Epoch [743/1000], Average Loss: 0.0061\n",
      "Epoch [744/1000], Average Loss: 0.0054\n",
      "Epoch [745/1000], Average Loss: 0.0061\n",
      "Epoch [746/1000], Average Loss: 0.0071\n",
      "Epoch [747/1000], Average Loss: 0.0059\n",
      "Epoch [748/1000], Average Loss: 0.0068\n",
      "Epoch [749/1000], Average Loss: 0.0054\n",
      "Epoch [750/1000], Average Loss: 0.0062\n",
      "Epoch [751/1000], Average Loss: 0.0067\n",
      "Epoch [752/1000], Average Loss: 0.0081\n",
      "Epoch [753/1000], Average Loss: 0.0063\n",
      "Epoch [754/1000], Average Loss: 0.0063\n",
      "Epoch [755/1000], Average Loss: 0.0062\n",
      "Epoch [756/1000], Average Loss: 0.0049\n",
      "Epoch [757/1000], Average Loss: 0.0054\n",
      "Epoch [758/1000], Average Loss: 0.0053\n",
      "Epoch [759/1000], Average Loss: 0.0057\n",
      "Epoch [760/1000], Average Loss: 0.0057\n",
      "Epoch [761/1000], Average Loss: 0.0053\n",
      "Epoch [762/1000], Average Loss: 0.0059\n",
      "Epoch [763/1000], Average Loss: 0.0058\n",
      "Epoch [764/1000], Average Loss: 0.0057\n",
      "Epoch [765/1000], Average Loss: 0.0054\n",
      "Epoch [766/1000], Average Loss: 0.0068\n",
      "Epoch [767/1000], Average Loss: 0.0071\n",
      "Epoch [768/1000], Average Loss: 0.0045\n",
      "Epoch [769/1000], Average Loss: 0.0054\n",
      "Epoch [770/1000], Average Loss: 0.0046\n",
      "Epoch [771/1000], Average Loss: 0.0061\n",
      "Epoch [772/1000], Average Loss: 0.0064\n",
      "Epoch [773/1000], Average Loss: 0.0052\n",
      "Epoch [774/1000], Average Loss: 0.0057\n",
      "Epoch [775/1000], Average Loss: 0.0063\n",
      "Epoch [776/1000], Average Loss: 0.0066\n",
      "Epoch [777/1000], Average Loss: 0.0082\n",
      "Epoch [778/1000], Average Loss: 0.0064\n",
      "Epoch [779/1000], Average Loss: 0.0053\n",
      "Epoch [780/1000], Average Loss: 0.0060\n",
      "Epoch [781/1000], Average Loss: 0.0065\n",
      "Epoch [782/1000], Average Loss: 0.0077\n",
      "Epoch [783/1000], Average Loss: 0.0050\n",
      "Epoch [784/1000], Average Loss: 0.0048\n",
      "Epoch [785/1000], Average Loss: 0.0053\n",
      "Epoch [786/1000], Average Loss: 0.0072\n",
      "Epoch [787/1000], Average Loss: 0.0073\n",
      "Epoch [788/1000], Average Loss: 0.0070\n",
      "Epoch [789/1000], Average Loss: 0.0079\n",
      "Epoch [790/1000], Average Loss: 0.0065\n",
      "Epoch [791/1000], Average Loss: 0.0055\n",
      "Epoch [792/1000], Average Loss: 0.0057\n",
      "Epoch [793/1000], Average Loss: 0.0061\n",
      "Epoch [794/1000], Average Loss: 0.0059\n",
      "Epoch [795/1000], Average Loss: 0.0066\n",
      "Epoch [796/1000], Average Loss: 0.0060\n",
      "Epoch [797/1000], Average Loss: 0.0066\n",
      "Epoch [798/1000], Average Loss: 0.0067\n",
      "Epoch [799/1000], Average Loss: 0.0050\n",
      "Epoch [800/1000], Average Loss: 0.0065\n",
      "Epoch [801/1000], Average Loss: 0.0061\n",
      "Epoch [802/1000], Average Loss: 0.0057\n",
      "Epoch [803/1000], Average Loss: 0.0070\n",
      "Epoch [804/1000], Average Loss: 0.0058\n",
      "Epoch [805/1000], Average Loss: 0.0059\n",
      "Epoch [806/1000], Average Loss: 0.0067\n",
      "Epoch [807/1000], Average Loss: 0.0073\n",
      "Epoch [808/1000], Average Loss: 0.0061\n",
      "Epoch [809/1000], Average Loss: 0.0051\n",
      "Epoch [810/1000], Average Loss: 0.0060\n",
      "Epoch [811/1000], Average Loss: 0.0067\n",
      "Epoch [812/1000], Average Loss: 0.0063\n",
      "Epoch [813/1000], Average Loss: 0.0054\n",
      "Epoch [814/1000], Average Loss: 0.0061\n",
      "Epoch [815/1000], Average Loss: 0.0056\n",
      "Epoch [816/1000], Average Loss: 0.0062\n",
      "Epoch [817/1000], Average Loss: 0.0057\n",
      "Epoch [818/1000], Average Loss: 0.0071\n",
      "Epoch [819/1000], Average Loss: 0.0053\n",
      "Epoch [820/1000], Average Loss: 0.0053\n",
      "Epoch [821/1000], Average Loss: 0.0047\n",
      "Epoch [822/1000], Average Loss: 0.0067\n",
      "Epoch [823/1000], Average Loss: 0.0074\n",
      "Epoch [824/1000], Average Loss: 0.0063\n",
      "Epoch [825/1000], Average Loss: 0.0055\n",
      "Epoch [826/1000], Average Loss: 0.0056\n",
      "Epoch [827/1000], Average Loss: 0.0051\n",
      "Epoch [828/1000], Average Loss: 0.0047\n",
      "Epoch [829/1000], Average Loss: 0.0061\n",
      "Epoch [830/1000], Average Loss: 0.0059\n",
      "Epoch [831/1000], Average Loss: 0.0050\n",
      "Epoch [832/1000], Average Loss: 0.0049\n",
      "Epoch [833/1000], Average Loss: 0.0056\n",
      "Epoch [834/1000], Average Loss: 0.0065\n",
      "Epoch [835/1000], Average Loss: 0.0059\n",
      "Epoch [836/1000], Average Loss: 0.0050\n",
      "Epoch [837/1000], Average Loss: 0.0047\n",
      "Epoch [838/1000], Average Loss: 0.0068\n",
      "Epoch [839/1000], Average Loss: 0.0045\n",
      "Epoch [840/1000], Average Loss: 0.0062\n",
      "Epoch [841/1000], Average Loss: 0.0056\n",
      "Epoch [842/1000], Average Loss: 0.0057\n",
      "Epoch [843/1000], Average Loss: 0.0062\n",
      "Epoch [844/1000], Average Loss: 0.0065\n",
      "Epoch [845/1000], Average Loss: 0.0071\n",
      "Epoch [846/1000], Average Loss: 0.0056\n",
      "Epoch [847/1000], Average Loss: 0.0054\n",
      "Epoch [848/1000], Average Loss: 0.0062\n",
      "Epoch [849/1000], Average Loss: 0.0055\n",
      "Epoch [850/1000], Average Loss: 0.0051\n",
      "Epoch [851/1000], Average Loss: 0.0062\n",
      "Epoch [852/1000], Average Loss: 0.0062\n",
      "Epoch [853/1000], Average Loss: 0.0063\n",
      "Epoch [854/1000], Average Loss: 0.0063\n",
      "Epoch [855/1000], Average Loss: 0.0047\n",
      "Epoch [856/1000], Average Loss: 0.0049\n",
      "Epoch [857/1000], Average Loss: 0.0052\n",
      "Epoch [858/1000], Average Loss: 0.0056\n",
      "Epoch [859/1000], Average Loss: 0.0052\n",
      "Epoch [860/1000], Average Loss: 0.0058\n",
      "Epoch [861/1000], Average Loss: 0.0056\n",
      "Epoch [862/1000], Average Loss: 0.0073\n",
      "Epoch [863/1000], Average Loss: 0.0078\n",
      "Epoch [864/1000], Average Loss: 0.0061\n",
      "Epoch [865/1000], Average Loss: 0.0067\n",
      "Epoch [866/1000], Average Loss: 0.0049\n",
      "Epoch [867/1000], Average Loss: 0.0056\n",
      "Epoch [868/1000], Average Loss: 0.0050\n",
      "Epoch [869/1000], Average Loss: 0.0045\n",
      "Epoch [870/1000], Average Loss: 0.0072\n",
      "Epoch [871/1000], Average Loss: 0.0058\n",
      "Epoch [872/1000], Average Loss: 0.0052\n",
      "Epoch [873/1000], Average Loss: 0.0060\n",
      "Epoch [874/1000], Average Loss: 0.0052\n",
      "Epoch [875/1000], Average Loss: 0.0099\n",
      "Epoch [876/1000], Average Loss: 0.0069\n",
      "Epoch [877/1000], Average Loss: 0.0056\n",
      "Epoch [878/1000], Average Loss: 0.0063\n",
      "Epoch [879/1000], Average Loss: 0.0050\n",
      "Epoch [880/1000], Average Loss: 0.0048\n",
      "Epoch [881/1000], Average Loss: 0.0058\n",
      "Epoch [882/1000], Average Loss: 0.0049\n",
      "Epoch [883/1000], Average Loss: 0.0053\n",
      "Epoch [884/1000], Average Loss: 0.0057\n",
      "Epoch [885/1000], Average Loss: 0.0049\n",
      "Epoch [886/1000], Average Loss: 0.0057\n",
      "Epoch [887/1000], Average Loss: 0.0065\n",
      "Epoch [888/1000], Average Loss: 0.0070\n",
      "Epoch [889/1000], Average Loss: 0.0087\n",
      "Epoch [890/1000], Average Loss: 0.0053\n",
      "Epoch [891/1000], Average Loss: 0.0052\n",
      "Epoch [892/1000], Average Loss: 0.0052\n",
      "Epoch [893/1000], Average Loss: 0.0054\n",
      "Epoch [894/1000], Average Loss: 0.0058\n",
      "Epoch [895/1000], Average Loss: 0.0063\n",
      "Epoch [896/1000], Average Loss: 0.0066\n",
      "Epoch [897/1000], Average Loss: 0.0053\n",
      "Epoch [898/1000], Average Loss: 0.0051\n",
      "Epoch [899/1000], Average Loss: 0.0043\n",
      "Epoch [900/1000], Average Loss: 0.0062\n",
      "Epoch [901/1000], Average Loss: 0.0054\n",
      "Epoch [902/1000], Average Loss: 0.0061\n",
      "Epoch [903/1000], Average Loss: 0.0059\n",
      "Epoch [904/1000], Average Loss: 0.0069\n",
      "Epoch [905/1000], Average Loss: 0.0056\n",
      "Epoch [906/1000], Average Loss: 0.0064\n",
      "Epoch [907/1000], Average Loss: 0.0054\n",
      "Epoch [908/1000], Average Loss: 0.0067\n",
      "Epoch [909/1000], Average Loss: 0.0060\n",
      "Epoch [910/1000], Average Loss: 0.0058\n",
      "Epoch [911/1000], Average Loss: 0.0048\n",
      "Epoch [912/1000], Average Loss: 0.0045\n",
      "Epoch [913/1000], Average Loss: 0.0050\n",
      "Epoch [914/1000], Average Loss: 0.0069\n",
      "Epoch [915/1000], Average Loss: 0.0064\n",
      "Epoch [916/1000], Average Loss: 0.0057\n",
      "Epoch [917/1000], Average Loss: 0.0053\n",
      "Epoch [918/1000], Average Loss: 0.0062\n",
      "Epoch [919/1000], Average Loss: 0.0050\n",
      "Epoch [920/1000], Average Loss: 0.0060\n",
      "Epoch [921/1000], Average Loss: 0.0068\n",
      "Epoch [922/1000], Average Loss: 0.0058\n",
      "Epoch [923/1000], Average Loss: 0.0065\n",
      "Epoch [924/1000], Average Loss: 0.0063\n",
      "Epoch [925/1000], Average Loss: 0.0054\n",
      "Epoch [926/1000], Average Loss: 0.0057\n",
      "Epoch [927/1000], Average Loss: 0.0044\n",
      "Epoch [928/1000], Average Loss: 0.0038\n",
      "Epoch [929/1000], Average Loss: 0.0045\n",
      "Epoch [930/1000], Average Loss: 0.0055\n",
      "Epoch [931/1000], Average Loss: 0.0065\n",
      "Epoch [932/1000], Average Loss: 0.0063\n",
      "Epoch [933/1000], Average Loss: 0.0054\n",
      "Epoch [934/1000], Average Loss: 0.0063\n",
      "Epoch [935/1000], Average Loss: 0.0042\n",
      "Epoch [936/1000], Average Loss: 0.0067\n",
      "Epoch [937/1000], Average Loss: 0.0057\n",
      "Epoch [938/1000], Average Loss: 0.0051\n",
      "Epoch [939/1000], Average Loss: 0.0046\n",
      "Epoch [940/1000], Average Loss: 0.0061\n",
      "Epoch [941/1000], Average Loss: 0.0056\n",
      "Epoch [942/1000], Average Loss: 0.0071\n",
      "Epoch [943/1000], Average Loss: 0.0059\n",
      "Epoch [944/1000], Average Loss: 0.0049\n",
      "Epoch [945/1000], Average Loss: 0.0073\n",
      "Epoch [946/1000], Average Loss: 0.0062\n",
      "Epoch [947/1000], Average Loss: 0.0068\n",
      "Epoch [948/1000], Average Loss: 0.0076\n",
      "Epoch [949/1000], Average Loss: 0.0052\n",
      "Epoch [950/1000], Average Loss: 0.0056\n",
      "Epoch [951/1000], Average Loss: 0.0051\n",
      "Epoch [952/1000], Average Loss: 0.0071\n",
      "Epoch [953/1000], Average Loss: 0.0059\n",
      "Epoch [954/1000], Average Loss: 0.0053\n",
      "Epoch [955/1000], Average Loss: 0.0053\n",
      "Epoch [956/1000], Average Loss: 0.0047\n",
      "Epoch [957/1000], Average Loss: 0.0047\n",
      "Epoch [958/1000], Average Loss: 0.0042\n",
      "Epoch [959/1000], Average Loss: 0.0048\n",
      "Epoch [960/1000], Average Loss: 0.0054\n",
      "Epoch [961/1000], Average Loss: 0.0059\n",
      "Epoch [962/1000], Average Loss: 0.0069\n",
      "Epoch [963/1000], Average Loss: 0.0064\n",
      "Epoch [964/1000], Average Loss: 0.0050\n",
      "Epoch [965/1000], Average Loss: 0.0066\n",
      "Epoch [966/1000], Average Loss: 0.0052\n",
      "Epoch [967/1000], Average Loss: 0.0049\n",
      "Epoch [968/1000], Average Loss: 0.0048\n",
      "Epoch [969/1000], Average Loss: 0.0073\n",
      "Epoch [970/1000], Average Loss: 0.0054\n",
      "Epoch [971/1000], Average Loss: 0.0058\n",
      "Epoch [972/1000], Average Loss: 0.0057\n",
      "Epoch [973/1000], Average Loss: 0.0056\n",
      "Epoch [974/1000], Average Loss: 0.0052\n",
      "Epoch [975/1000], Average Loss: 0.0072\n",
      "Epoch [976/1000], Average Loss: 0.0059\n",
      "Epoch [977/1000], Average Loss: 0.0058\n",
      "Epoch [978/1000], Average Loss: 0.0057\n",
      "Epoch [979/1000], Average Loss: 0.0059\n",
      "Epoch [980/1000], Average Loss: 0.0057\n",
      "Epoch [981/1000], Average Loss: 0.0044\n",
      "Epoch [982/1000], Average Loss: 0.0060\n",
      "Epoch [983/1000], Average Loss: 0.0046\n",
      "Epoch [984/1000], Average Loss: 0.0047\n",
      "Epoch [985/1000], Average Loss: 0.0069\n",
      "Epoch [986/1000], Average Loss: 0.0064\n",
      "Epoch [987/1000], Average Loss: 0.0051\n",
      "Epoch [988/1000], Average Loss: 0.0054\n",
      "Epoch [989/1000], Average Loss: 0.0066\n",
      "Epoch [990/1000], Average Loss: 0.0060\n",
      "Epoch [991/1000], Average Loss: 0.0047\n",
      "Epoch [992/1000], Average Loss: 0.0066\n",
      "Epoch [993/1000], Average Loss: 0.0053\n",
      "Epoch [994/1000], Average Loss: 0.0056\n",
      "Epoch [995/1000], Average Loss: 0.0050\n",
      "Epoch [996/1000], Average Loss: 0.0067\n",
      "Epoch [997/1000], Average Loss: 0.0060\n",
      "Epoch [998/1000], Average Loss: 0.0055\n",
      "Epoch [999/1000], Average Loss: 0.0051\n",
      "Epoch [1000/1000], Average Loss: 0.0067\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNE0lEQVR4nO3deVxU9f4/8NfMwAzrDPuwiKJg4pJiIohLtlBotmhW5rVE6qc3l65F3ZtmqdU1NMtri0lZ2p5L39TKJRU1s8hdU1PcEnAZFhGGdYCZz+8P4+gEbnjgwPB6Ph7zeDjnfM7Mew4ILz7LOSohhAARERGRg1ArXQARERGRnBhuiIiIyKEw3BAREZFDYbghIiIih8JwQ0RERA6F4YaIiIgcCsMNERERORSGGyIiInIoDDdERETkUBhuiJqQUaNGISwsrF7HTp8+HSqVSt6CiOrwySefQKVSYefOnUqXQlQnhhuia6BSqa7psXnzZqVLVcSoUaPg4eGhdBkOoyY8XO7x22+/KV0iUZPmpHQBRM3B559/bvf8s88+w/r162tt79ix4w29z4IFC2Cz2ep17EsvvYRJkybd0PtT0/Lqq6+ibdu2tbZHREQoUA1R88FwQ3QNHnvsMbvnv/32G9avX19r+9+VlZXBzc3tmt/H2dm5XvUBgJOTE5yc+F+6uSgtLYW7u/sV2wwcOBDR0dGNVBGR4+CwFJFMbrvtNnTp0gW7du3CrbfeCjc3N7z44osAgJUrV2LQoEEIDg6GTqdDeHg4XnvtNVitVrvX+Pucm5MnT0KlUuHNN9/Ehx9+iPDwcOh0OvTs2RM7duywO7auOTcqlQoTJkzAihUr0KVLF+h0OnTu3Blr166tVf/mzZsRHR0NFxcXhIeH44MPPpB9Hs+yZcvQo0cPuLq6ws/PD4899hhOnz5t18ZkMiEpKQmtWrWCTqdDUFAQHnjgAZw8eVJqs3PnTiQkJMDPzw+urq5o27YtnnjiiWuq4f3330fnzp2h0+kQHByM8ePHo7CwUNo/YcIEeHh4oKysrNaxw4cPR2BgoN3Xbc2aNejXrx/c3d3h6emJQYMG4eDBg3bH1QzbHT9+HPfccw88PT0xYsSIa6r3Si79/vjf//6HNm3awNXVFf3798eBAwdqtd+4caNUq5eXFx544AEcOnSoVrvTp0/jySeflL5f27Zti7Fjx6KystKuncViQXJyMvz9/eHu7o4hQ4YgLy/Prs2NfK2I6ot/5hHJ6Ny5cxg4cCAeffRRPPbYYzAajQAuzKHw8PBAcnIyPDw8sHHjRkydOhVmsxmzZ8++6ut+9dVXKC4uxj//+U+oVCq88cYbePDBB3HixImr9vZs3boV3377LcaNGwdPT0+88847GDp0KLKysuDr6wsA2LNnDwYMGICgoCC88sorsFqtePXVV+Hv73/jJ+Uvn3zyCZKSktCzZ0+kpKQgJycHb7/9Nn755Rfs2bMHXl5eAIChQ4fi4MGDePrppxEWFobc3FysX78eWVlZ0vO7774b/v7+mDRpEry8vHDy5El8++23V61h+vTpeOWVVxAfH4+xY8ciIyMD8+fPx44dO/DLL7/A2dkZw4YNw7x587Bq1So8/PDD0rFlZWX4/vvvMWrUKGg0GgAXhisTExORkJCAWbNmoaysDPPnz0ffvn2xZ88eu6BaXV2NhIQE9O3bF2+++eY19egVFRUhPz/fbptKpZK+bjU+++wzFBcXY/z48aioqMDbb7+NO+64A/v375e+Bzds2ICBAweiXbt2mD59OsrLy/Huu++iT58+2L17t1TrmTNnEBMTg8LCQowZMwaRkZE4ffo0vvnmG5SVlUGr1Urv+/TTT8Pb2xvTpk3DyZMnMXfuXEyYMAFLliwBgBv6WhHdEEFE1238+PHi7/99+vfvLwCI1NTUWu3LyspqbfvnP/8p3NzcREVFhbQtMTFRtGnTRnr+559/CgDC19dXFBQUSNtXrlwpAIjvv/9e2jZt2rRaNQEQWq1WHDt2TNq2b98+AUC8++670rb77rtPuLm5idOnT0vbjh49KpycnGq9Zl0SExOFu7v7ZfdXVlaKgIAA0aVLF1FeXi5t/+GHHwQAMXXqVCGEEOfPnxcAxOzZsy/7WsuXLxcAxI4dO65a16Vyc3OFVqsVd999t7BardL29957TwAQCxcuFEIIYbPZREhIiBg6dKjd8UuXLhUAxJYtW4QQQhQXFwsvLy8xevRou3Ymk0kYDAa77YmJiQKAmDRp0jXVumjRIgGgzodOp5Pa1Xx/uLq6ilOnTknbt23bJgCIZ599VtoWFRUlAgICxLlz56Rt+/btE2q1WowcOVLaNnLkSKFWq+s8vzabza6++Ph4aZsQQjz77LNCo9GIwsJCIUT9v1ZEN4rDUkQy0ul0SEpKqrXd1dVV+ndxcTHy8/PRr18/lJWV4fDhw1d93WHDhsHb21t63q9fPwDAiRMnrnpsfHw8wsPDpeddu3aFXq+XjrVardiwYQMGDx6M4OBgqV1ERAQGDhx41de/Fjt37kRubi7GjRsHFxcXafugQYMQGRmJVatWAbhwnrRaLTZv3ozz58/X+Vo1PTw//PADqqqqrrmGDRs2oLKyEs888wzU6os/+kaPHg29Xi/VoFKp8PDDD2P16tUoKSmR2i1ZsgQhISHo27cvAGD9+vUoLCzE8OHDkZ+fLz00Gg1iY2OxadOmWjWMHTv2musFgHnz5mH9+vV2jzVr1tRqN3jwYISEhEjPY2JiEBsbi9WrVwMAzp49i71792LUqFHw8fGR2nXt2hV33XWX1M5ms2HFihW477776pzr8/chyjFjxtht69evH6xWKzIzMwHU/2tFdKMYbohkFBISYtdtX+PgwYMYMmQIDAYD9Ho9/P39pcnIRUVFV33d1q1b2z2vCTqXCwBXOrbm+Jpjc3NzUV5eXucKHLlW5dT8suvQoUOtfZGRkdJ+nU6HWbNmYc2aNTAajbj11lvxxhtvwGQySe379++PoUOH4pVXXoGfnx8eeOABLFq0CBaLpV41aLVatGvXTtoPXAiT5eXl+O677wAAJSUlWL16NR5++GHpl/nRo0cBAHfccQf8/f3tHuvWrUNubq7d+zg5OaFVq1ZXP1mXiImJQXx8vN3j9ttvr9Wuffv2tbbddNNN0jylK53/jh07Ij8/H6WlpcjLy4PZbEaXLl2uqb6rfV/W92tFdKMYbohkdGkPTY3CwkL0798f+/btw6uvvorvv/8e69evx6xZswDgmpZ+18zx+DshRIMeq4RnnnkGR44cQUpKClxcXPDyyy+jY8eO2LNnD4ALvQfffPMN0tPTMWHCBJw+fRpPPPEEevToYdfTciN69eqFsLAwLF26FADw/fffo7y8HMOGDZPa1HzdPv/881q9K+vXr8fKlSvtXlOn09n1GDmCq31vNcbXiqgujvU/jagJ2rx5M86dO4dPPvkEEydOxL333ov4+Hi7YSYlBQQEwMXFBceOHau1r65t9dGmTRsAQEZGRq19GRkZ0v4a4eHheO6557Bu3TocOHAAlZWVeOutt+za9OrVCzNmzMDOnTvx5Zdf4uDBg1i8ePF111BZWYk///yzVg2PPPII1q5dC7PZjCVLliAsLAy9evWyqxG4cP7+3rsSHx+P22677SpnRT41vUiXOnLkiDRJ+Ern//Dhw/Dz84O7uzv8/f2h1+vrXGl1I673a0V0oxhuiBpYzV+3l/aUVFZW4v3331eqJDsajQbx8fFYsWIFzpw5I20/duxYnfM76iM6OhoBAQFITU21G5JYs2YNDh06hEGDBgG4sCKpoqLC7tjw8HB4enpKx50/f75Wr1NUVBQAXHG4Iz4+HlqtFu+8847d8R9//DGKioqkGmoMGzYMFosFn376KdauXYtHHnnEbn9CQgL0ej1ef/31OueT/H1JdENasWKF3ZL67du3Y9u2bdKcqaCgIERFReHTTz+1W/Z+4MABrFu3Dvfccw8AQK1WY/Dgwfj+++/rvLXC9fb21fdrRXSjuBScqIH17t0b3t7eSExMxL/+9S+oVCp8/vnnTWpYaPr06Vi3bh369OmDsWPHwmq14r333kOXLl2wd+/ea3qNqqoq/Pe//6213cfHB+PGjcOsWbOQlJSE/v37Y/jw4dJS8LCwMDz77LMALvQ23HnnnXjkkUfQqVMnODk5Yfny5cjJycGjjz4KAPj000/x/vvvY8iQIQgPD0dxcTEWLFgAvV4v/ZKui7+/PyZPnoxXXnkFAwYMwP3334+MjAy8//776NmzZ60LMt5yyy2IiIjAlClTYLFY7IakAECv12P+/Pl4/PHHccstt+DRRx+Fv78/srKysGrVKvTp0wfvvffeNZ27y1mzZk2dE8579+6Ndu3aSc8jIiLQt29fjB07FhaLBXPnzoWvry/+85//SG1mz56NgQMHIi4uDk8++aS0FNxgMGD69OlSu9dffx3r1q1D//79MWbMGHTs2BFnz57FsmXLsHXrVmmS8LWo79eK6IYptk6LqBm73FLwzp0719n+l19+Eb169RKurq4iODhY/Oc//xE//vijACA2bdoktbvcUvC6lkYDENOmTZOeX24p+Pjx42sd26ZNG5GYmGi3LS0tTXTv3l1otVoRHh4uPvroI/Hcc88JFxeXy5yFi2qWOtf1CA8Pl9otWbJEdO/eXeh0OuHj4yNGjBhht4Q5Pz9fjB8/XkRGRgp3d3dhMBhEbGysWLp0qdRm9+7dYvjw4aJ169ZCp9OJgIAAce+994qdO3detU4hLiz9joyMFM7OzsJoNIqxY8eK8+fP19l2ypQpAoCIiIi47Ott2rRJJCQkCIPBIFxcXER4eLgYNWqUXT1XWyr/d1daCg5ALFq0SAhh//3x1ltvidDQUKHT6US/fv3Evn37ar3uhg0bRJ8+fYSrq6vQ6/XivvvuE3/88UetdpmZmWLkyJHC399f6HQ60a5dOzF+/HhhsVjs6vv7Eu9NmzbZfU/f6NeKqL5UQjShPx+JqEkZPHgwDh48WOecDlLeyZMn0bZtW8yePRvPP/+80uUQNRmcc0NEAIDy8nK750ePHsXq1asbdWIsEZEcOOeGiAAA7dq1w6hRo6RrvsyfPx9ardZu3gYRUXPAcENEAIABAwbg66+/hslkgk6nQ1xcHF5//fU6LxBHRNSUcc4NERERORTOuSEiIiKHwnBDREREDqXFzbmx2Ww4c+YMPD09a93hloiIiJomIQSKi4sRHBx81fu0tbhwc+bMGYSGhipdBhEREdVDdnY2WrVqdcU2LS7ceHp6ArhwcvR6vcLVEBER0bUwm80IDQ2Vfo9fSYsLNzVDUXq9nuGGiIiombmWKSWcUExEREQOheGGiIiIHArDDRERETkUhhsiIiJyKAw3RERE5FAYboiIiMihMNwQERGRQ2G4ISIiIofCcENEREQOheGGiIiIHArDDRERETkUhhsiIiJyKC3uxpkNxVJtRV6xBU5qNQINLkqXQ0RE1GKx50YmB06b0XfWJjzyQbrSpRAREbVoDDcyExBKl0BERNSiMdzIRKVSugIiIiICGG5kJ9hxQ0REpCiGG5nUdNww3BARESmL4UYmKo5LERERNQkMN0RERORQGG5kwn4bIiKipoHhRmaCk26IiIgUxXAjE065ISIiahoYbmTGfhsiIiJlMdzIRPXXrBuOShERESmL4UYmHJYiIiJqGhhuZMZ7SxERESmL4YaIiIgcCsONzDjnhoiISFkMNzKpmXPDbENERKQshhuZqHiNYiIioiaB4UZmHJYiIiJSFsONTLgUnIiIqGlguJEdu26IiIiUxHAjE/bcEBERNQ0MNzLh7ReIiIiaBoYbmTHbEBERKYvhRiYcliIiImoaGG5kJjguRUREpCiGG5mw44aIiKhpYLiRCW+/QERE1DQoHm7mzZuHsLAwuLi4IDY2Ftu3b79i+8LCQowfPx5BQUHQ6XS46aabsHr16kaq9uo4KkVERKQsJyXffMmSJUhOTkZqaipiY2Mxd+5cJCQkICMjAwEBAbXaV1ZW4q677kJAQAC++eYbhISEIDMzE15eXo1ffC0cmCIiImoKFA03c+bMwejRo5GUlAQASE1NxapVq7Bw4UJMmjSpVvuFCxeioKAAv/76K5ydnQEAYWFhjVnyVXFCMRERkbIUG5aqrKzErl27EB8ff7EYtRrx8fFIT0+v85jvvvsOcXFxGD9+PIxGI7p06YLXX38dVqu1scq+LC4FJyIiahoU67nJz8+H1WqF0Wi02240GnH48OE6jzlx4gQ2btyIESNGYPXq1Th27BjGjRuHqqoqTJs2rc5jLBYLLBaL9NxsNsv3IerAfhsiIiJlKT6h+HrYbDYEBATgww8/RI8ePTBs2DBMmTIFqamplz0mJSUFBoNBeoSGhjZIbey4ISIiahoUCzd+fn7QaDTIycmx256Tk4PAwMA6jwkKCsJNN90EjUYjbevYsSNMJhMqKyvrPGby5MkoKiqSHtnZ2fJ9iEuouBaciIioSVAs3Gi1WvTo0QNpaWnSNpvNhrS0NMTFxdV5TJ8+fXDs2DHYbDZp25EjRxAUFAStVlvnMTqdDnq93u7RkJhtiIiIlKXosFRycjIWLFiATz/9FIcOHcLYsWNRWloqrZ4aOXIkJk+eLLUfO3YsCgoKMHHiRBw5cgSrVq3C66+/jvHjxyv1ESQcliIiImoaFF0KPmzYMOTl5WHq1KkwmUyIiorC2rVrpUnGWVlZUKsv5q/Q0FD8+OOPePbZZ9G1a1eEhIRg4sSJeOGFF5T6CLVwKTgREZGyVKKF/TY2m80wGAwoKiqSdYgq81wp+s/eDHetBgdfHSDb6xIREdH1/f5uVqulmjLVXwNTLSopEhERNUEMNzJrWf1gRERETQ/DjUx4hWIiIqKmgeFGZoIDU0RERIpiuCEiIiKHwnAjE+kCxey4ISIiUhTDjcyYbYiIiJTFcCMTFWcUExERNQkMN3Jj1w0REZGiGG5kwn4bIiKipoHhRibShGJ23RARESmK4YaIiIgcCsONTKR7S7HjhoiISFEMNzJjtiEiIlIWw41MuBKciIioaWC4kZnguBQREZGiGG5kwo4bIiKipoHhRi7SUnAiIiJSEsONzDgqRUREpCyGG5moODBFRETUJDDcEBERkUNhuJEJl4ITERE1DQw3Mrk023A5OBERkXIYboiIiMihMNzIRHXJuBQ7boiIiJTDcNMAmG2IiIiUw3AjE84nJiIiahoYbmRy6WopTigmIiJSDsMNERERORSGG5lceoVi9tsQEREph+GmAXBUioiISDkMN3LhjGIiIqImgeFGJnYTijkwRUREpBiGGyIiInIoDDcysb+3lGJlEBERtXgMN0RERORQGG5kcum9pYiIiEg5DDcNgMNSREREymG4kQn7bYiIiJoGhhuZcCk4ERFR09Akws28efMQFhYGFxcXxMbGYvv27Zdt+8knn0ClUtk9XFxcGrFaIiIiasoUDzdLlixBcnIypk2bht27d6Nbt25ISEhAbm7uZY/R6/U4e/as9MjMzGzEiutmd28pdtwQEREpRvFwM2fOHIwePRpJSUno1KkTUlNT4ebmhoULF172GJVKhcDAQOlhNBobseKrY7YhIiJSjqLhprKyErt27UJ8fLy0Ta1WIz4+Hunp6Zc9rqSkBG3atEFoaCgeeOABHDx4sDHKvSKuBCciImoaFA03+fn5sFqttXpejEYjTCZTncd06NABCxcuxMqVK/HFF1/AZrOhd+/eOHXqVJ3tLRYLzGaz3aOhCY5LERERKUbxYanrFRcXh5EjRyIqKgr9+/fHt99+C39/f3zwwQd1tk9JSYHBYJAeoaGhjVwxERERNSZFw42fnx80Gg1ycnLstufk5CAwMPCaXsPZ2Rndu3fHsWPH6tw/efJkFBUVSY/s7Owbrrsu9kvBiYiISCmKhhutVosePXogLS1N2maz2ZCWloa4uLhreg2r1Yr9+/cjKCiozv06nQ56vd7u0dA4KkVERKQcJ6ULSE5ORmJiIqKjoxETE4O5c+eitLQUSUlJAICRI0ciJCQEKSkpAIBXX30VvXr1QkREBAoLCzF79mxkZmbi//2//6fkx7BbCk5ERETKUTzcDBs2DHl5eZg6dSpMJhOioqKwdu1aaZJxVlYW1OqLHUznz5/H6NGjYTKZ4O3tjR49euDXX39Fp06dlPoIAP62Woo9N0RERIpRiRa2tMdsNsNgMKCoqEjWIaoqqw3tp6wBAOybejcMbs6yvTYREVFLdz2/v5vdaqmmyr7jpkXlRSIioiaF4YaIiIgcCsONTFQq3luKiIioKWC4kQnnExMRETUNDDdERETkUBhuZGJ3hWKOSxERESmG4YaIiIgcCsONTOwmFCtYBxERUUvHcNMAOCpFRESkHIYbIiIicigMNzKqGZniFYqJiIiUw3BDREREDoXhRkbSlGJ23BARESmG4aYBMNsQEREph+FGRpcuByciIiJlMNzIqCbacCk4ERGRchhuiIiIyKEw3MiIS8GJiIiUx3BDREREDoXhRkaqv2bdcM4NERGRchhu5CQNSxEREZFSGG6IiIjIoTDcyOjiUnD23RARESmF4YaIiIgcCsONjKSl4Oy4ISIiUgzDjYxU4O0XiIiIlMZwQ0RERA6F4UZGHJYiIiJSHsMNERERORSGGxlJS8F5GT8iIiLFMNwQERGRQ2G4kZFKxXtLERERKY3hRkYXh6WIiIhIKQw3RERE5FAYbuQkLQVn3w0REZFSGG6IiIjIoTDcyIhzboiIiJTHcCMjrpYiIiJSHsMNERERORSGGxmppJuCs+uGiIhIKU0i3MybNw9hYWFwcXFBbGwstm/ffk3HLV68GCqVCoMHD27YAomIiKjZUDzcLFmyBMnJyZg2bRp2796Nbt26ISEhAbm5uVc87uTJk3j++efRr1+/Rqr06qQJxey4ISIiUozi4WbOnDkYPXo0kpKS0KlTJ6SmpsLNzQ0LFy687DFWqxUjRozAK6+8gnbt2jVitVcmTShWuA4iIqKWTNFwU1lZiV27diE+Pl7aplarER8fj/T09Mse9+qrryIgIABPPvlkY5RJREREzYiTkm+en58Pq9UKo9Fot91oNOLw4cN1HrN161Z8/PHH2Lt37zW9h8VigcVikZ6bzeZ613s1HJYiIiJSnuLDUtejuLgYjz/+OBYsWAA/P79rOiYlJQUGg0F6hIaGNnCVREREpCRFe278/Pyg0WiQk5Njtz0nJweBgYG12h8/fhwnT57EfffdJ22z2WwAACcnJ2RkZCA8PNzumMmTJyM5OVl6bjabGyzg1CwFF5x1Q0REpBhFw41Wq0WPHj2QlpYmLee22WxIS0vDhAkTarWPjIzE/v377ba99NJLKC4uxttvv11naNHpdNDpdA1Sf22qqzchIiKiBqVouAGA5ORkJCYmIjo6GjExMZg7dy5KS0uRlJQEABg5ciRCQkKQkpICFxcXdOnSxe54Ly8vAKi1XUmcc0NERKQcxcPNsGHDkJeXh6lTp8JkMiEqKgpr166VJhlnZWVBrW4eU4OkYSmGGyIiIsWohGhZv4rNZjMMBgOKioqg1+tlfe2eMzYgr9iC1f/qh07B8r42ERFRS3Y9v7+bR5dIMyEtBeeEYiIiIsUw3BAREZFDYbiREefcEBERKY/hRkYqLgUnIiJSHMMNERERORSGGxlxWIqIiEh5DDdERETkUBhuZMSl4ERERMpjuJGRSsUJxUREREpjuGkAnHNDRESkHIabBsBsQ0REpByGGyIiInIoDDcyurgUnH03RERESmG4kRHnExMRESmP4aYBsN+GiIhIOQw3Mqq5txRHpYiIiJRTr3CTnZ2NU6dOSc+3b9+OZ555Bh9++KFshRERERHVR73CzT/+8Q9s2rQJAGAymXDXXXdh+/btmDJlCl599VVZC2xOLs65YdcNERGRUuoVbg4cOICYmBgAwNKlS9GlSxf8+uuv+PLLL/HJJ5/IWV+zwvnEREREyqtXuKmqqoJOpwMAbNiwAffffz8AIDIyEmfPnpWvumaKc26IiIiUU69w07lzZ6SmpuLnn3/G+vXrMWDAAADAmTNn4OvrK2uBzUnNvaWYbYiIiJRTr3Aza9YsfPDBB7jtttswfPhwdOvWDQDw3XffScNVREREREpwqs9Bt912G/Lz82E2m+Ht7S1tHzNmDNzc3GQrrrmpmXPDYSkiIiLl1Kvnpry8HBaLRQo2mZmZmDt3LjIyMhAQECBrgc0KZxQTEREprl7h5oEHHsBnn30GACgsLERsbCzeeustDB48GPPnz5e1wOaI95YiIiJSTr3Cze7du9GvXz8AwDfffAOj0YjMzEx89tlneOedd2QtsDlhxw0REZHy6hVuysrK4OnpCQBYt24dHnzwQajVavTq1QuZmZmyFtgcsd+GiIhIOfUKNxEREVixYgWys7Px448/4u677wYA5ObmQq/Xy1pgcyItBWe6ISIiUky9ws3UqVPx/PPPIywsDDExMYiLiwNwoRene/fushZIREREdD3qtRT8oYceQt++fXH27FnpGjcAcOedd2LIkCGyFdfcSEvBOTBFRESkmHqFGwAIDAxEYGCgdHfwVq1atfgL+Kk4o5iIiEhx9RqWstlsePXVV2EwGNCmTRu0adMGXl5eeO2112Cz2eSusflhxw0REZFi6tVzM2XKFHz88ceYOXMm+vTpAwDYunUrpk+fjoqKCsyYMUPWIpsLFXhvKSIiIqXVK9x8+umn+Oijj6S7gQNA165dERISgnHjxrXYcENERETKq9ewVEFBASIjI2ttj4yMREFBwQ0X1VzVzLnhUnAiIiLl1CvcdOvWDe+9916t7e+99x66du16w0URERER1Ve9hqXeeOMNDBo0CBs2bJCucZOeno7s7GysXr1a1gKbIy4FJyIiUk69em769++PI0eOYMiQISgsLERhYSEefPBBHDx4EJ9//rncNTYbKq4FJyIiUly9r3MTHBxca+Lwvn378PHHH+PDDz+84cKaM865ISIiUk69em6obhevUExERERKYbiREUeliIiIlNckws28efMQFhYGFxcXxMbGYvv27Zdt++233yI6OhpeXl5wd3dHVFRUk5vnIzguRUREpJjrmnPz4IMPXnF/YWHhdRewZMkSJCcnIzU1FbGxsZg7dy4SEhKQkZGBgICAWu19fHwwZcoUREZGQqvV4ocffkBSUhICAgKQkJBw3e8vJ/bcEBERKe+6wo3BYLjq/pEjR15XAXPmzMHo0aORlJQEAEhNTcWqVauwcOFCTJo0qVb72267ze75xIkT8emnn2Lr1q2Kh5sa7LchIiJSznWFm0WLFsn65pWVldi1axcmT54sbVOr1YiPj0d6evpVjxdCYOPGjcjIyMCsWbPqbGOxWGCxWKTnZrP5xgu/jJp7SzHdEBERKUfROTf5+fmwWq0wGo12241GI0wm02WPKyoqgoeHB7RaLQYNGoR3330Xd911V51tU1JSYDAYpEdoaKisn+FSHJYiIiJSXpOYUHy9PD09sXfvXuzYsQMzZsxAcnIyNm/eXGfbyZMno6ioSHpkZ2c3eH28QjEREZFy6n0RPzn4+flBo9EgJyfHbntOTg4CAwMve5xarUZERAQAICoqCocOHUJKSkqt+TgAoNPpoNPpZK37cthxQ0REpDxFe260Wi169OiBtLQ0aZvNZkNaWpp0z6prYbPZ7ObVKI0rwYmIiJSjaM8NACQnJyMxMRHR0dGIiYnB3LlzUVpaKq2eGjlyJEJCQpCSkgLgwhya6OhohIeHw2KxYPXq1fj8888xf/58JT/GBX9NumG4ISIiUo7i4WbYsGHIy8vD1KlTYTKZEBUVhbVr10qTjLOysqBWX+xgKi0txbhx43Dq1Cm4uroiMjISX3zxBYYNG6bURyAiIqImRCVa2OV0zWYzDAYDioqKoNfrZX3twfN+wd7sQiwYGY27OhmvfgARERFdk+v5/d0sV0s1VVwKTkREpDyGmwbQwjrDiIiImhSGGxmx44aIiEh5DDcNgP02REREymG4kZGKS8GJiIgUx3AjI7U0LsV0Q0REpBSGGxnV9NzYmG2IiIgUw3Ajo5qeGyvTDRERkWIYbmSklnpuGG6IiIiUwnAjI42aE4qJiIiUxnAjIxV7boiIiBTHcCOjmjk3nHJDRESkHIYbGUlzbphuiIiIFMNwI6OLPTcMN0REREphuJGRmte5ISIiUhzDjYy4FJyIiEh5DDcyUv91NgXDDRERkWIYbmRUsxScVygmIiJSDsONjDjnhoiISHkMNzLScLUUERGR4hhuZFTTc8NsQ0REpByGGxnx9gtERETKY7iRUc1F/KwMN0RERIphuJERh6WIiIiUx3AjI7Wa95YiIiJSGsONjHhXcCIiIuUx3MiIt18gIiJSHsONjHhXcCIiIuUx3MhImnPDcENERKQYhhsZ8fYLREREymO4kRGHpYiIiJTHcCMjXueGiIhIeQw3Mqq5/YKV41JERESKYbiRkeavs8lhKSIiIuUw3MiIw1JERETKY7iREe8KTkREpDyGGxlxtRQREZHyGG5kpJYmFCtcCBERUQvGcCMjjbpmzg17boiIiJTCcCMjFYeliIiIFNckws28efMQFhYGFxcXxMbGYvv27Zdtu2DBAvTr1w/e3t7w9vZGfHz8Fds3Jt5+gYiISHmKh5slS5YgOTkZ06ZNw+7du9GtWzckJCQgNze3zvabN2/G8OHDsWnTJqSnpyM0NBR33303Tp8+3ciV18YJxURERMpTPNzMmTMHo0ePRlJSEjp16oTU1FS4ublh4cKFdbb/8ssvMW7cOERFRSEyMhIfffQRbDYb0tLSGrny2qSeG3bdEBERKUbRcFNZWYldu3YhPj5e2qZWqxEfH4/09PRreo2ysjJUVVXBx8enzv0WiwVms9nu0VA4LEVERKQ8RcNNfn4+rFYrjEaj3Xaj0QiTyXRNr/HCCy8gODjYLiBdKiUlBQaDQXqEhobecN2Xw2EpIiIi5Sk+LHUjZs6cicWLF2P58uVwcXGps83kyZNRVFQkPbKzsxusHrWat18gIiJSmpOSb+7n5weNRoOcnBy77Tk5OQgMDLzisW+++SZmzpyJDRs2oGvXrpdtp9PpoNPpZKn3anj7BSIiIuUp2nOj1WrRo0cPu8nANZOD4+LiLnvcG2+8gddeew1r165FdHR0Y5R6TWqGpaycdENERKQYRXtuACA5ORmJiYmIjo5GTEwM5s6di9LSUiQlJQEARo4ciZCQEKSkpAAAZs2ahalTp+Krr75CWFiYNDfHw8MDHh4ein0OANBwQjEREZHiFA83w4YNQ15eHqZOnQqTyYSoqCisXbtWmmSclZUFtfpiB9P8+fNRWVmJhx56yO51pk2bhunTpzdm6bXUrJbi7ReIiIiUo3i4AYAJEyZgwoQJde7bvHmz3fOTJ082fEH1xNsvEBERKa9Zr5ZqanidGyIiIuUx3MioZvSMPTdERETKYbiRkZpLwYmIiBTHcCOji/eWUrgQIiKiFozhRkbsuSEiIlIew42Mai7ix2xDRESkHIYbGWn+SjdVHJciIiJSDMONjJydLpzOKivDDRERkVIYbmSk01w4nZXVDDdERERKYbiR0cWeG066ISIiUgrDjYy07LkhIiJSHMONjJxrwg3n3BARESmG4UZGWk4oJiIiUhzDjYw4LEVERKQ8hhsZOTv9dZ0b9twQEREphuFGRjU9N1VWAZuNK6aIiIiUwHAjo5ql4ACvUkxERKQUhhsZ1fTcAJx3Q0REpBSGGxldGm54IT8iIiJlMNzISK1Wwemvm2ey54aIiEgZDDcyc9bwWjdERERKYriRmbPmQs+NhT03REREimC4kZnWSQOAPTdERERKYbiRmVbDOTdERERKYriRGe8vRUREpCyGG5nxzuBERETKYriRWU3PDYeliIiIlMFwIzPnS+4vRURERI2P4UZmNVcpZs8NERGRMhhuZMYJxURERMpiuJGZM5eCExERKYrhRmbShGL23BARESmC4UZmzpxzQ0REpCiGG5lxzg0REZGyGG5kxtVSREREymK4kdnF69ww3BARESmB4UZmFycU8yJ+RERESmC4kRknFBMRESmL4UZmnFBMRESkLIYbmWl5ET8iIiJFKR5u5s2bh7CwMLi4uCA2Nhbbt2+/bNuDBw9i6NChCAsLg0qlwty5cxuv0GvEnhsiIiJlKRpulixZguTkZEybNg27d+9Gt27dkJCQgNzc3Drbl5WVoV27dpg5cyYCAwMbudprUzPnxsJwQ0REpAhFw82cOXMwevRoJCUloVOnTkhNTYWbmxsWLlxYZ/uePXti9uzZePTRR6HT6Rq52msjLQXnsBQREZEiFAs3lZWV2LVrF+Lj4y8Wo1YjPj4e6enpsr2PxWKB2Wy2ezQk3luKiIhIWYqFm/z8fFitVhiNRrvtRqMRJpNJtvdJSUmBwWCQHqGhobK9dl20vIgfERGRohSfUNzQJk+ejKKiIumRnZ3doO8n9dxwWIqIiEgRTkq9sZ+fHzQaDXJycuy25+TkyDpZWKfTNer8HOkifrxCMRERkSIU67nRarXo0aMH0tLSpG02mw1paWmIi4tTqqwbxp4bIiIiZSnWcwMAycnJSExMRHR0NGJiYjB37lyUlpYiKSkJADBy5EiEhIQgJSUFwIVJyH/88Yf079OnT2Pv3r3w8PBARESEYp/jUs5/XcSPc26IiIiUoWi4GTZsGPLy8jB16lSYTCZERUVh7dq10iTjrKwsqNUXO5fOnDmD7t27S8/ffPNNvPnmm+jfvz82b97c2OXXSceL+BERESlK0XADABMmTMCECRPq3Pf3wBIWFgYhmvZcFt44k4iISFkOv1qqsfH2C0RERMpiuJGZdPsF9twQEREpguFGZryIHxERkbIYbmTGpeBERETKYriRWU3PjU0AVlvTnvxMRETkiBhuZObsdPGUsveGiIio8THcyKym5wbgncGJiIiUwHAjs5orFAPsuSEiIlICw43MVCqV1HtjqbYqXA0REVHLw3DTAAL0F+5CfqawQuFKiIiIWh6GmwYQ7u8BADiaW6xwJURERC0Pw00DiAi4EG4++eVkk78XFhERkaNhuGkAI2JbAwCO5pbg1PlyhashIiJqWRhuGkA7fw90CdEDAA6eKVK4GiIiopaF4aaBdA4yAAAOnDYrXAkREVHLwnDTQDqz54aIiEgRDDcNpHPwhXDzx1n23BARETUmhpsGEuLlBgDIL6mEjTfQJCIiajQMNw3Ey80ZwIU7gxdXVCtcDRERUcvBcNNAXJw1cNNqAADnyyoVroaIiKjlYLhpQN5uWgBAZkGZwpUQERG1HAw3Deh04YUL+CUu3M4rFRMRETUShpsGFObrJv17U0augpUQERG1HAw3DWjuo92lf3+9PRvmiioFqyEiImoZGG4aUFSoFxaN6gkAWP9HDrpOX4fle04pXBUREZFjY7hpYN7uWrvn8zcfV6gSIiKiloHhpoH5/i3cuDhrFKqEiIioZWC4aWB/77n5/VQR/r1sH349lq9QRURERI6N4aaBuWtr99Qs23UK//hoG7adOKdARURERI6N4aaBqVQqRAZ6AgCS+oTZ7Rv24W944L2tWLHnNFbsOV3va+FsOpyLFXtO32ipREREDkElWtjV5cxmMwwGA4qKiqDX6xvlPU8XliO/2IJuoV44db4MfWdtqrNdjzbeCPZyRZivG57s2xa5xRbcZPTEsdwS/N/uU5hwewTcdU52x9hsAu1eXA0A2PLv29H6kmvrEBEROYrr+f3tdMW9JIsQL1eEeLkCAFp5u+HwawMQ+fLaWu12ZZ7HrszzAIB3Nx4DACwf1xvjv9yNM0UVOHjGjI5Bnri/WzA6BxsAAIXlF6+dc6aonOGGiIhaPA5LKcDFWYNn4tujU9DVe46GvP8rzhRVAAC2HMnDBz+dQOLCHcj+635VecUWqe26gzkQQsBSbUV2QRlyiyuu+vo2W4vquCMiohaAw1IKO5FXgle+/wM/Hcm77mPXPXsrkhbtkO5hBQCvD7kZK/eexrY/C+Cm1eCbp3qjU3Ddn3PtAROSl+7F7Ie6YVDXoOt+//wSC/afKsJtHfyhUqmu+3giIqJrdT2/vxlumogvt2XiwOki3N05EKfPl+OlFQdked12/u5IS+4vhY8zheXYeDgXA7oEIvq/G6R2P//ndvh76uCsUeNITjGqrDYEeLog0OBy2ddO+N8WZOQUY+6wKAzuHmK37z/f7MPpwnJ8nNhTurZPfokFn/56Eom9w+DnoZPl8xERUcvAcHMFTTXcXEoIgTUHTGjl7YqNh3Mxd8NRjL89HDonDeasP3Ldr+ekVkGlAqqsV/5SB+pd0NrXDdv/LAAARAZ64t6uQTh4xoxzJZUI9nKBwIWA9NHInuj26joAwK03+eP9Ebcg7VAOEjoHwiYEOk39EQDwv2HdMKR7KxSUVuKW19YDAO7qZMSCkdFXrdtqE1i8Iwv9Ivyb1Fwiq01Ao2ZPFRFRY2K4uYLmEG4uVW214bCpGJ2D9VCpVNiVeR5D5/8KAHBxVqOiygYA0Ls4wVxRLR03PKY1vt6e1ej19o3wg5tWg3V/5EjbAvUuMJnt5/8sGtUT2/4sQOdgPQINLli6Ixt3dw7E1JUH8Ex8ezzUIxSLfvkT/111CF5uztjz8l1QqVTYnXUebloNDp8tRnwnIzz+tnqsJnhUW21QqVRYtjMbfh46xHcyXrHuDFMx9K5O8NA5wdPF+bLtpizfj+/2ncEXT8bisMmMQV2D4aFzQqmlGm5aDVQqFWw2AfVVwk9FlRX5JRa08r720Lbhjxyk/nQck+/pCGeNCl1beUEIgZ+O5CHc3wOhPk0nABIRyY3h5gqaW7i5nF2ZBfB20+LX4+ewL7sQ/x3SBdkFZbj33a3QqFRYM/FWrD+Ug9d++KPWse0DPJBbbEHRXyut+t/kX685P43t0jB3JW183ZB5rsxu292djOjfwR/Hc0vx40ETEjoHYsIdEZi36RhW/X5WCl86JzUWjIxGTFsfHMstQYdATzhr1Kiy2rD2gAlPf73H7nV93LVwddbgdGE5/hHbGk/0aYthH6SjZ5gPnrv7Jmw9lo+3047i3eHd0TnYgNk/HsbX27Ol4921Grz5cDcMvPnCnKctR/JwJKcYj8e1wey1GdhyNA+dgw0I8XLFe5uOScepVEBiXBjWHjDBZK7AzSEGfP90X1iqrTieW4pOwXocOmvGwLd/BgAsHtMLvdr5AgAqq214dsletPJxxeSBHevxlbgxNT9yVCoVyiqrUVBaeV0hr7m4MLnfZnfLlaU7srEpIxfT7+8MF2cNTp0vg7+HDgH6yw//kmOpstpgLq+CL4fmrxvDzRU4Sri5nLLKaqiggutfV0Y+W1SOF7/dj3tuDkIbX3eoVEDPMB8AQOa5UliqbX9dS6cYvx4/B7VKBYOrMzoEeuLlFQegUavw0qBO6BjkiS+2ZWHz4Vw8fWd7DJ73S53vH9/RiA2Hcurc99bD3fDcsn0N88FlZnB1lsJfY3B11qDSaoNVptVrM4Z0wZTl9vO23LUalFZa7ba9O7w7CkorcdhkxqbDeVLIe7xXG9wRGYAFP5+AzkmNjkF6vP/XTV/7RPji9SE3o42vO+ZvPo5fj+fj4BkzvN2c8cHj0RBC4GxRBaxCIMTLFe46J1itAkbDhR/mQ+f/ivJKK+aNuAVjv9iNzHOliAv3RY7Zgif6tMWJvBIcNhXj4ehWsFTb4O+pw08ZeThTWI5/9m8HANC7OKO98cLFMSuqrJi28iBubmVAp2A9Fm79E/+6sz3aB3hgZ+Z5RAZ6wtPFGWeLyvHHGTMCDS7Yf6oIR3NL0ClIj/ujglFltcEmAA+dE8wVVcg1WxAR4IGyymqUV1rh66FDeaUVWQVl2HIkDyoV8HhcG+ic7K9AXlxRhaU7T+H+bsF4a10Glu85jf8Ni8I9f4XXsEmran2tNGoVxtzaDvnFFrx4T0ecyC/Fr8fy8f/6tZP+HwshcCK/FH7uOhjc7HsWz5dWYuPhXNxzcxBctRoIIaBSqXA0pxgLfzmJcH93DI9pDZ2TWpp7Z6m2wk17sdcz11yB3VnnYSqqwMPRobWup3Wpa+mZvB7HcouRfqIA/4hpLQ33nsgrga+HDgbXy/ei1jh1vgzllVa0N3qi2mqDk+bCIuCC0kr4/O32N42t1FINjVplF3CnrTyAL7ZlYcmYXoj+62cxcOF756Of/8R93YIQEeDZ4LU1x+F1hpsrcPRw01jKK61QqS4saz+aU4xJ3+7Hc3ffhN7hfhBCYHdWISIDPWH7699RoV4wuDrj898y8f6mY/h3QgfYBLDuoEn6Qbq8jqss65zUsFTboNWocVcnI47kFONobgmACwGkosoKS7V9b05jBZPOwXq0D/DA9j8LpOX61Hg8dU7QuzrbrRa8nH7t/fDz0Wu7n5vWSY3K6to9hM4aVa15ay7OalhtAqN6h2HBz39e9jW7hOgxul87TFy895pqqBFkcIG/pw42IXDgtFnqsfv9VCF2ZxXaDfn2a++HGYNvxjNL9iCroBz5JZZar3fp/417bg7Eoz1b44+zZry38RhKLBeHtZ+/+yacLizH8bxS9An3Q4Beh3B/D3y1LRM//H4WXUIMuDnEgDUHzqJvhB/UahVCvd3QtZUBH245geN5JRh/ewTCfN1xJKcYKWsOI1DvgvZGDxj1Lqiy2lBYVgWdk1oawg7xcsXpwnL0aOONPVnnoXd1xtJ/xiHM1x0V1VYUlFQi/cQ5/HjQhDG3tsPZwgq4aTWYvHw/isqr0LOND/ZmF6Jfez+kHc4FAPwjtjW6tTLg56P5cHHWwNPFCcfzSuGkVuFcaSV6tfOBCheuIp9dUIYNh3LwQFQIFu/IQqcgPd54qBsOnilCG193bDqcCx8PLbYezUf7AA/4e+qwcu8Z3GT0QO8IPxSVVaGwvBLRbXwQ6uOGb3efQvLSC3/MfT26FzZl5CLHXIGVe89I53nv1LvgrnNCtVVg2Ifp+P1UEaJCvbBgZDRW7j2NW9p4AwC2nSjAvuxCuGo16BjkiYd6hOKL3zIBANFtvHE0twSbMnIx6OYg3Ns1GNU2G5buPAWtkxrHcopxe2QASizVSP3pOA6cNgO48MfObR0C4K7T4LXBXaBz0iDzXCne33QcNiHQt70fvvgtE73a+eKW1t74YMtxjOodhshAPTRqFdYeMKG4ogqeLs4orqhCK283aNQqVFpt+L9dp5DUpy3uuTlQ1pW0zS7czJs3D7Nnz4bJZEK3bt3w7rvvIiYm5rLtly1bhpdffhknT55E+/btMWvWLNxzzz3X9F4MN03fobNmVFbb0NbfHXoXZwghYBMX/sIVQmDzkTxEt/Guc26MzSYgAPyZX4qC0krEtL3wl5Gl2oqKShsyC0qxO/M80g7nosRSDUuVDW5aDSbcEYGKKiuCDK54d+NRqFQqjLstHL+dKMDSndnQuzhhYnx7tPZxw86T55FVUIZn77oJzn/9lSiEwMy1h3G2sALuOid8u/sUHogKRnFFNYbHtIaPuxaf/noSy3adglajxuqJ/aB3cYKl2oZ/fPQbsgvK4euuxbnSSgBARIAHht7SCt1CDYgI8MDX27LRJ8IXJnMFwnzd8cGWE/h+34Ufkn0j/LC1id2ItbWPG/KKLSivsl69MRHZUamAxvzN7OehQ2SgJ34/VWg3d/NGtPZxw4bk/tA6yXc5vWYVbpYsWYKRI0ciNTUVsbGxmDt3LpYtW4aMjAwEBATUav/rr7/i1ltvRUpKCu6991589dVXmDVrFnbv3o0uXbpc9f0YbqipsVRboVappKB0LYQQKK+6OLRQZbXhs/RMnC0sR3SYD47lFsOod0GojxtUAGLa+uDkuTJ46JygVgEeLk6w2gQOm4rh5eqMdv4eAC4M8SzYcgI3tzLg4BkzRsS2hpebVhqecdc5ocpqw5Yj+Rj/1W4AwBsPdUVsWx94u2tRUlGNQL2LNGxxLLcEzy3dC1etBq890AXlVVZ4u2mRX2LB0ZwSdG/tBXNFNfw8tNC7OCO/xII2vu4or7Ji5prDOF1YjnZ+7oht64NuoV74LD0T5ZXVeKJvW6zYcwZLd2ajtY8bbuvgj292nUL/m/wRaHBB6k/HkV9SaXfOAjx1aOvnDj9PHQ6dMWPCHRHY/mcBlu06hbcfjYKHzgk/HjQhIsAT3+87g1berugYpMfxvBKYiioQ7u8BtQpIP3EOce18sTPzPKw2gSqrDcfzSu3ey1PnhC4hBoT5uUsT+z1dnBAT5oOoUC9sOZoHH3ctfNx1eKp/O5wvq8Inv/yJFXvPoHOwHhVVVrvXvK9bMP55azus+yMHPx/Nw77sQvx9BLO1jxvOFJaj2iYwoHMgerXzwX3dguHirMF/Vx1CXrEFgQYdvtl1qtbcNWeNCm183XEstwQP92iFZbtOXfZ7r1c7H2SdK7vu3soJt0egxFKN/9t9Cs4aNZzUKliqbRh6SyukHc6xmyfn666FSoVaX8O61LVgoXOwHl5uzvjl2Dn4eWiv6XWU5qbVoKzScf4YSH3sFgzocv3XT7uSZhVuYmNj0bNnT7z33nsAAJvNhtDQUDz99NOYNGlSrfbDhg1DaWkpfvjhB2lbr169EBUVhdTU1Ku+H8MNkXwKSivh7ebcJC/iWGKpRq65QgpuDa1mrktd6pp7cS0qq21X/cs3v8QCrZMaehdnlFiqUVZZjQDPK09Q3ptdiOKKKvh56BDi7QontcpuDk5hWSV2Z53HLa29oXPSoKLKioycYnQJMcBD5wQhBArLqiAAqABU2wTKKqux8+R5PBAVjLwSC3zdddA6qZFdUIZAg4tdL+ffz1O11YZSixUGN2cUlVXB4OYMq01gy5GaEKiFl5szqq0CmzJysWRHNp66LRytfdwQ7u+BwrJKaNQq7D9VhPNlVYjvFACdkwallmpp2DvDVAybEOj415XhbTaBA2eK0DFIj7JKK9y1GhRXVGNTRi7a+LqhU5ABPx3JRXnVhT8+OgfrUVhWha+2ZyG6jQ8e6tEKzhoVTheWo8oq4KxRobzSip+O5OEmoyda+7ghzM8d50osWHvQhJyiCtx6kz+0TmqE+bmjsLQKq/afhZtWA293LbqHesHg5nxhXpjeBUFeLli8PRtniyow7vZwuGudYKm2wmoT+Cw9Ex46Jwzt0QqZ50pRbRXoFuqF3OIKvPjtARzNLUbKkJtRXmXF/+0+hT4Rfqi2CgyOCsGWo3m4u7MRWefKYDJXoH2AJzYcysGWI3m4OcSAUX3C8MuxfBRXVEMASOgciBJLNZw1KgR4umDtgbOY/9MJ3NreD/3a+8PHXYv04/nwdtfC4OqMjkF6nCupRIdA+ecNNZtwU1lZCTc3N3zzzTcYPHiwtD0xMRGFhYVYuXJlrWNat26N5ORkPPPMM9K2adOmYcWKFdi3r/ZkVYvFAovl4tiz2WxGaGgoww0REVEzcj3hRtF7S+Xn58NqtcJotL8GidFohMlkqvMYk8l0Xe1TUlJgMBikR2hoqDzFExERUZPk8DfOnDx5MoqKiqRHdnb21Q8iIiKiZuvyFzNoBH5+ftBoNMjJsb8uSk5ODgIDA+s8JjAw8Lra63Q66HS8WBIREVFLoWjPjVarRY8ePZCWliZts9lsSEtLQ1xcXJ3HxMXF2bUHgPXr11+2PREREbUsivbcAEBycjISExMRHR2NmJgYzJ07F6WlpUhKSgIAjBw5EiEhIUhJSQEATJw4Ef3798dbb72FQYMGYfHixdi5cyc+/PBDJT8GERERNRGKh5thw4YhLy8PU6dOhclkQlRUFNauXStNGs7KyoJafbGDqXfv3vjqq6/w0ksv4cUXX0T79u2xYsWKa7rGDRERETk+xa9z09h4nRsiIqLmp9ksBSciIiKSG8MNERERORSGGyIiInIoDDdERETkUBhuiIiIyKEw3BAREZFDYbghIiIih6L4RfwaW81lfcxms8KVEBER0bWq+b19LZfna3Hhpri4GAAQGhqqcCVERER0vYqLi2EwGK7YpsVdodhms+HMmTPw9PSESqWS9bXNZjNCQ0ORnZ3Nqx83IJ7nxsHz3Hh4rhsHz3PjaKjzLIRAcXExgoOD7W7LVJcW13OjVqvRqlWrBn0PvV7P/ziNgOe5cfA8Nx6e68bB89w4GuI8X63HpgYnFBMREZFDYbghIiIih8JwIyOdTodp06ZBp9MpXYpD43luHDzPjYfnunHwPDeOpnCeW9yEYiIiInJs7LkhIiIih8JwQ0RERA6F4YaIiIgcCsMNERERORSGG5nMmzcPYWFhcHFxQWxsLLZv3650Sc1KSkoKevbsCU9PTwQEBGDw4MHIyMiwa1NRUYHx48fD19cXHh4eGDp0KHJycuzaZGVlYdCgQXBzc0NAQAD+/e9/o7q6ujE/SrMyc+ZMqFQqPPPMM9I2nmd5nD59Go899hh8fX3h6uqKm2++GTt37pT2CyEwdepUBAUFwdXVFfHx8Th69KjdaxQUFGDEiBHQ6/Xw8vLCk08+iZKSksb+KE2a1WrFyy+/jLZt28LV1RXh4eF47bXX7O4/xHN9/bZs2YL77rsPwcHBUKlUWLFihd1+uc7p77//jn79+sHFxQWhoaF444035PkAgm7Y4sWLhVarFQsXLhQHDx4Uo0ePFl5eXiInJ0fp0pqNhIQEsWjRInHgwAGxd+9ecc8994jWrVuLkpISqc1TTz0lQkNDRVpamti5c6fo1auX6N27t7S/urpadOnSRcTHx4s9e/aI1atXCz8/PzF58mQlPlKTt337dhEWFia6du0qJk6cKG3neb5xBQUFok2bNmLUqFFi27Zt4sSJE+LHH38Ux44dk9rMnDlTGAwGsWLFCrFv3z5x//33i7Zt24ry8nKpzYABA0S3bt3Eb7/9Jn7++WcREREhhg8frsRHarJmzJghfH19xQ8//CD+/PNPsWzZMuHh4SHefvttqQ3P9fVbvXq1mDJlivj2228FALF8+XK7/XKc06KiImE0GsWIESPEgQMHxNdffy1cXV3FBx98cMP1M9zIICYmRowfP156brVaRXBwsEhJSVGwquYtNzdXABA//fSTEEKIwsJC4ezsLJYtWya1OXTokAAg0tPThRAX/jOq1WphMpmkNvPnzxd6vV5YLJbG/QBNXHFxsWjfvr1Yv3696N+/vxRueJ7l8cILL4i+fftedr/NZhOBgYFi9uzZ0rbCwkKh0+nE119/LYQQ4o8//hAAxI4dO6Q2a9asESqVSpw+fbrhim9mBg0aJJ544gm7bQ8++KAYMWKEEILnWg5/DzdyndP3339feHt72/3ceOGFF0SHDh1uuGYOS92gyspK7Nq1C/Hx8dI2tVqN+Ph4pKenK1hZ81ZUVAQA8PHxAQDs2rULVVVVduc5MjISrVu3ls5zeno6br75ZhiNRqlNQkICzGYzDh482IjVN33jx4/HoEGD7M4nwPMsl++++w7R0dF4+OGHERAQgO7du2PBggXS/j///BMmk8nuPBsMBsTGxtqdZy8vL0RHR0tt4uPjoVarsW3btsb7ME1c7969kZaWhiNHjgAA9u3bh61bt2LgwIEAeK4bglznND09Hbfeeiu0Wq3UJiEhARkZGTh//vwN1djibpwpt/z8fFitVrsf9ABgNBpx+PBhhapq3mw2G5555hn06dMHXbp0AQCYTCZotVp4eXnZtTUajTCZTFKbur4ONfvogsWLF2P37t3YsWNHrX08z/I4ceIE5s+fj+TkZLz44ovYsWMH/vWvf0Gr1SIxMVE6T3Wdx0vPc0BAgN1+Jycn+Pj48DxfYtKkSTCbzYiMjIRGo4HVasWMGTMwYsQIAOC5bgBynVOTyYS2bdvWeo2afd7e3vWukeGGmpzx48fjwIED2Lp1q9KlOJzs7GxMnDgR69evh4uLi9LlOCybzYbo6Gi8/vrrAIDu3bvjwIEDSE1NRWJiosLVOZalS5fiyy+/xFdffYXOnTtj7969eOaZZxAcHMxz3YJxWOoG+fn5QaPR1FpNkpOTg8DAQIWqar4mTJiAH374AZs2bUKrVq2k7YGBgaisrERhYaFd+0vPc2BgYJ1fh5p9dGHYKTc3F7fccgucnJzg5OSEn376Ce+88w6cnJxgNBp5nmUQFBSETp062W3r2LEjsrKyAFw8T1f6uREYGIjc3Fy7/dXV1SgoKOB5vsS///1vTJo0CY8++ihuvvlmPP7443j22WeRkpICgOe6Ich1ThvyZwnDzQ3SarXo0aMH0tLSpG02mw1paWmIi4tTsLLmRQiBCRMmYPny5di4cWOtrsoePXrA2dnZ7jxnZGQgKytLOs9xcXHYv3+/3X+o9evXQ6/X1/pF01Ldeeed2L9/P/bu3Ss9oqOjMWLECOnfPM83rk+fPrUuZXDkyBG0adMGANC2bVsEBgbanWez2Yxt27bZnefCwkLs2rVLarNx40bYbDbExsY2wqdoHsrKyqBW2/8q02g0sNlsAHiuG4Jc5zQuLg5btmxBVVWV1Gb9+vXo0KHDDQ1JAeBScDksXrxY6HQ68cknn4g//vhDjBkzRnh5edmtJqErGzt2rDAYDGLz5s3i7Nmz0qOsrExq89RTT4nWrVuLjRs3ip07d4q4uDgRFxcn7a9Zonz33XeLvXv3irVr1wp/f38uUb6KS1dLCcHzLIft27cLJycnMWPGDHH06FHx5ZdfCjc3N/HFF19IbWbOnCm8vLzEypUrxe+//y4eeOCBOpfSdu/eXWzbtk1s3bpVtG/fvkUvT65LYmKiCAkJkZaCf/vtt8LPz0/85z//kdrwXF+/4uJisWfPHrFnzx4BQMyZM0fs2bNHZGZmCiHkOaeFhYXCaDSKxx9/XBw4cEAsXrxYuLm5cSl4U/Luu++K1q1bC61WK2JiYsRvv/2mdEnNCoA6H4sWLZLalJeXi3Hjxglvb2/h5uYmhgwZIs6ePWv3OidPnhQDBw4Urq6uws/PTzz33HOiqqqqkT9N8/L3cMPzLI/vv/9edOnSReh0OhEZGSk+/PBDu/02m028/PLLwmg0Cp1OJ+68806RkZFh1+bcuXNi+PDhwsPDQ+j1epGUlCSKi4sb82M0eWazWUycOFG0bt1auLi4iHbt2okpU6bYLS/mub5+mzZtqvNncmJiohBCvnO6b98+0bdvX6HT6URISIiYOXOmLPWrhLjkMo5EREREzRzn3BAREZFDYbghIiIih8JwQ0RERA6F4YaIiIgcCsMNERERORSGGyIiInIoDDdERETkUBhuiKjFU6lUWLFihdJlEJFMGG6ISFGjRo2CSqWq9RgwYIDSpRFRM+WkdAFERAMGDMCiRYvstul0OoWqIaLmjj03RKQ4nU6HwMBAu0fNXYFVKhXmz5+PgQMHwtXVFe3atcM333xjd/z+/ftxxx13wNXVFb6+vhgzZgxKSkrs2ixcuBCdO3eGTqdDUFAQJkyYYLc/Pz8fQ4YMgZubG9q3b4/vvvuuYT80ETUYhhsiavJefvllDB06FPv27cOIESPw6KOP4tChQwCA0tJSJCQkwNvbGzt27MCyZcuwYcMGu/Ayf/58jB8/HmPGjMH+/fvx3XffISIiwu49XnnlFTzyyCP4/fffcc8992DEiBEoKCho1M9JRDKR5fabRET1lJiYKDQajXB3d7d7zJgxQwhx4Y7xTz31lN0xsbGxYuzYsUIIIT788EPh7e0tSkpKpP2rVq0SarVamEwmIYQQwcHBYsqUKZetAYB46aWXpOclJSUCgFizZo1sn5OIGg/n3BCR4m6//XbMnz/fbpuPj4/077i4OLt9cXFx2Lt3LwDg0KFD6NatG9zd3aX9ffr0gc1mQ0ZGBlQqFc6cOYM777zzijV07dpV+re7uzv0ej1yc3Pr+5GISEEMN0SkOHd391rDRHJxdXW9pnbOzs52z1UqFWw2W0OUREQNjHNuiKjJ++2332o979ixIwCgY8eO2LdvH0pLS6X9v/zyC9RqNTp06ABPT0+EhYUhLS2tUWsmIuWw54aIFGexWGAymey2OTk5wc/PDwCwbNkyREdHo2/fvvjyyy+xfft2fPzxxwCAESNGYNq0aUhMTMT06dORl5eHp59+Go8//jiMRiMAYPr06XjqqacQEBCAgQMHori4GL/88guefvrpxv2gRNQoGG6ISHFr165FUFCQ3bYOHTrg8OHDAC6sZFq8eDHGjRuHoKAgfP311+jUqRMAwM3NDT/++CMmTpyInj17ws3NDUOHDsWcOXOk10pMTERFRQX+97//4fnnn4efnx8eeuihxvuARNSoVEIIoXQRRESXo1KpsHz5cgwePFjpUoiomeCcGyIiInIoDDdERETkUDjnhoiaNI6cE9H1Ys8NERERORSGGyIiInIoDDdERETkUBhuiIiIyKEw3BAREZFDYbghIiIih8JwQ0RERA6F4YaIiIgcCsMNEREROZT/D/3MdobQeqxRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set parameters\n",
    "input_size = len(cols) - 1\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "class Build_Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Build_Model, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)  # Adds a time dimension of size 1\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take only the last output for each sequence\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Pass through dense layers\n",
    "        output = self.dense_layers(lstm_out)\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "model = Build_Model(input_size=input_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "loss_values = []\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_features, batch_labels in train:\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average loss per epoch\n",
    "    avg_loss = epoch_loss / len(train)\n",
    "    loss_values.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled MSE Loss: 0.063171364\n",
      "RMSE Re-Scaled:  0.25133914213072384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Collect predictions and targets\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "# Inference on validation/test data\n",
    "with torch.no_grad():\n",
    "    for batch_features, batch_labels in test:\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # Store predictions and true values\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        targets.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Rescale predictions and targets back to original units\n",
    "predictions_rescaled = flu_df.target_scaler.inverse_transform(predictions)\n",
    "targets_rescaled = flu_df.target_scaler.inverse_transform(targets)\n",
    "\n",
    "# Calculate MSE loss on the rescaled data\n",
    "mse_loss_rescaled = np.mean((predictions_rescaled - targets_rescaled) ** 2)\n",
    "print(\"Rescaled MSE Loss:\", mse_loss_rescaled)\n",
    "print(\"RMSE Re-Scaled: \", m.sqrt(mse_loss_rescaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
